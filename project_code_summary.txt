Project Code Summary
Generated from: /home/lukas/aicoding/deep_learning
==================================================


==================== START OF FILE: CMakeLists.txt ====================
cmake_minimum_required(VERSION 3.18)
project(my_deep_lib LANGUAGES CXX CUDA)

# 设置 C++ 和 CUDA 标准
set(CMAKE_CXX_STANDARD 17)
set(CMAKE_CXX_STANDARD_REQUIRED ON)
set(CMAKE_CUDA_STANDARD 17)
set(CMAKE_CUDA_STANDARD_REQUIRED ON)

# 查找依赖
find_package(CUDAToolkit REQUIRED)
find_package(Python3 REQUIRED COMPONENTS Interpreter Development NumPy)

# 使用 pip 安装的 pybind11
execute_process(
    COMMAND ${Python3_EXECUTABLE} -c "import pybind11; print(pybind11.get_cmake_dir())"
    OUTPUT_VARIABLE pybind11_DIR
    OUTPUT_STRIP_TRAILING_WHITESPACE
)
find_package(pybind11 CONFIG REQUIRED)

# CUDA 架构设置（根据你的 GPU 调整）
# RTX 40xx: 89, RTX 30xx: 86, RTX 20xx: 75, GTX 10xx: 61
set(CMAKE_CUDA_ARCHITECTURES 75 86)

# 头文件目录
include_directories(
    ${CMAKE_SOURCE_DIR}/cuda_src
    ${Python3_INCLUDE_DIRS}
    ${Python3_NumPy_INCLUDE_DIRS}
)

# CUDA 源文件
set(CUDA_SOURCES
    cuda_src/tensor.cu
    cuda_src/conv_layer.cu
    cuda_src/layers.cu
    cuda_src/activations.cu
    cuda_src/elementwise.cu
)

# Pybind11 模块
pybind11_add_module(my_deep_lib
    ${CUDA_SOURCES}
    pybind/pybind_wrapper.cpp
)

# 链接库
target_link_libraries(my_deep_lib PRIVATE
    CUDA::cudart
    CUDA::cublas
)

# 设置输出目录
set_target_properties(my_deep_lib PROPERTIES
    LIBRARY_OUTPUT_DIRECTORY ${CMAKE_SOURCE_DIR}/python
    RUNTIME_OUTPUT_DIRECTORY ${CMAKE_SOURCE_DIR}/python
)

# 编译选项
target_compile_options(my_deep_lib PRIVATE
    $<$<COMPILE_LANGUAGE:CUDA>:
        -O3
        --expt-relaxed-constexpr
        -Xcompiler -fPIC
    >
)
==================== END OF FILE: CMakeLists.txt ====================

==================== START OF FILE: packer.py ====================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
项目代码导出脚本 - XML格式（LLM优化版）
"""

import os
from pathlib import Path
from datetime import datetime
import xml.sax.saxutils as saxutils

# ==================== 配置区域 ====================
OUTPUT_FILENAME = "project_export.xml"

# 包含的文件扩展名
INCLUDE_EXTENSIONS = {
    '.py', '.js', '.jsx', '.ts', '.tsx', '.java', '.c', '.cpp', '.h', '.hpp',
    '.cs', '.go', '.rs', '.php', '.rb', '.swift', '.kt', '.scala',
    '.html', '.css', '.scss', '.sass', '.less', '.vue', '.svelte',
    '.json', '.xml', '.yaml', '.yml', '.toml', '.ini', '.cfg', '.conf',
    '.md', '.txt', '.rst', '.sh', '.bash', '.sql', '.r', '.dart',
    '.gradle', '.properties', '.env', '.dockerfile'
}

# 排除的文件夹
EXCLUDE_DIRS = {
    'node_modules', 'venv', 'env', '.venv', '.env',
    '.git', '.svn', '.hg', '__pycache__', '.pytest_cache',
    'dist', 'build', 'target', 'out', 'output',
    '.idea', '.vscode', '.vs', '.settings',
    'bin', 'obj', 'pkg', 'vendor',
    '.next', '.nuxt', 'coverage', '.nyc_output',
    '.gradle', '.cache', '.mypy_cache', '.tox'
}

# 排除的文件
EXCLUDE_FILES = {
    '.DS_Store', 'Thumbs.db', 'desktop.ini',
    '.gitignore', '.gitkeep', '.dockerignore',
    OUTPUT_FILENAME,
    'package-lock.json', 'yarn.lock', 'poetry.lock', 'Pipfile.lock'
}

# 无扩展名但需要包含的特殊文件
SPECIAL_FILES = {
    'Makefile', 'Dockerfile', 'Rakefile', 'Gemfile', 
    'Procfile', 'Vagrantfile', 'Jenkinsfile'
}


def xml_escape(text):
    """XML转义，处理特殊字符"""
    if text is None:
        return ""
    return saxutils.escape(str(text))


def should_include_file(file_path):
    """判断文件是否应该被包含"""
    file_path = Path(file_path)
    
    # 排除特定文件
    if file_path.name in EXCLUDE_FILES:
        return False
    
    # 包含特殊文件
    if file_path.name in SPECIAL_FILES:
        return True
    
    # 检查扩展名
    return file_path.suffix.lower() in INCLUDE_EXTENSIONS


def generate_tree_node(path, prefix="", is_last=True, parent_chain=None):
    """
    递归生成目录树的XML节点
    
    Args:
        path: 当前路径
        prefix: 显示用的前缀（用于树形结构）
        is_last: 是否是同级最后一个
        parent_chain: 父路径链（用于检测循环）
    
    Returns:
        (tree_lines, file_nodes) - 树形结构行列表和文件节点列表
    """
    if parent_chain is None:
        parent_chain = set()
    
    # 防止循环引用（符号链接）
    try:
        real_path = path.resolve()
        if real_path in parent_chain:
            return [], []
    except (OSError, RuntimeError):
        return [], []
    
    tree_lines = []
    file_nodes = []
    
    # 读取目录内容
    try:
        items = list(path.iterdir())
    except PermissionError:
        return [f"{prefix}[Permission Denied]"], []
    except Exception as e:
        return [f"{prefix}[Error: {str(e)}]"], []
    
    # 过滤并排序：文件夹在前，然后按名称排序
    items = [
        item for item in items
        if item.name not in EXCLUDE_DIRS and item.name not in EXCLUDE_FILES
    ]
    items.sort(key=lambda x: (not x.is_dir(), x.name.lower()))
    
    # 更新父路径链
    new_chain = parent_chain | {real_path}
    
    # 遍历所有项目
    for index, item in enumerate(items):
        is_last_item = (index == len(items) - 1)
        connector = "└── " if is_last_item else "├── "
        
        if item.is_dir():
            # 处理文件夹
            tree_lines.append(f"{prefix}{connector}{item.name}/")
            
            # 递归处理子目录
            extension = "    " if is_last_item else "│   "
            sub_tree, sub_files = generate_tree_node(
                item, 
                prefix + extension, 
                is_last_item,
                new_chain
            )
            tree_lines.extend(sub_tree)
            file_nodes.extend(sub_files)
        else:
            # 处理文件
            tree_lines.append(f"{prefix}{connector}{item.name}")
            
            # 如果是需要包含的代码文件，添加到文件列表
            if should_include_file(item):
                file_nodes.append(item)
    
    return tree_lines, file_nodes


def read_file_content(file_path):
    """
    读取文件内容，尝试多种编码
    
    Returns:
        (content, encoding, error) - 文件内容、使用的编码、错误信息
    """
    encodings = ['utf-8', 'utf-8-sig', 'gbk', 'gb2312', 'gb18030', 'latin-1', 'cp1252']
    
    for encoding in encodings:
        try:
            with open(file_path, 'r', encoding=encoding) as f:
                content = f.read()
                return content, encoding, None
        except UnicodeDecodeError:
            continue
        except Exception as e:
            return None, None, f"Read error: {str(e)}"
    
    return None, None, "Cannot decode with any known encoding"


def get_file_info(file_path, root_path):
    """获取文件的详细信息"""
    try:
        stat = file_path.stat()
        relative_path = file_path.relative_to(root_path)
        
        return {
            'path': str(relative_path),
            'name': file_path.name,
            'extension': file_path.suffix or 'none',
            'size': stat.st_size,
            'modified': datetime.fromtimestamp(stat.st_mtime).isoformat()
        }
    except Exception as e:
        return None


def export_project_xml(root_path=None, output_file=None):
    """
    导出项目为XML格式
    
    Args:
        root_path: 项目根路径（默认当前目录）
        output_file: 输出文件名
    """
    if root_path is None:
        root_path = os.getcwd()
    
    if output_file is None:
        output_file = OUTPUT_FILENAME
    
    root = Path(root_path).resolve()
    output_path = root / output_file
    
    print(f"=" * 60)
    print(f"项目代码导出 - XML格式")
    print(f"=" * 60)
    print(f"项目路径: {root}")
    print(f"输出文件: {output_path}")
    print(f"=" * 60)
    
    # 生成目录树和收集文件
    print("\n[1/3] 扫描项目结构...")
    tree_lines, code_files = generate_tree_node(root)
    
    # 按路径排序文件
    code_files.sort(key=lambda x: str(x.relative_to(root)).lower())
    
    print(f"✓ 找到 {len(code_files)} 个代码文件")
    
    # 统计文件类型
    stats = {}
    total_size = 0
    for file_path in code_files:
        ext = file_path.suffix or 'no_extension'
        stats[ext] = stats.get(ext, 0) + 1
        try:
            total_size += file_path.stat().st_size
        except:
            pass
    
    # 写入XML文件
    print(f"\n[2/3] 生成XML文档...")
    
    with open(output_path, 'w', encoding='utf-8') as out:
        # XML头部
        out.write('<?xml version="1.0" encoding="UTF-8"?>\n')
        out.write('<project>\n\n')
        
        # ============ 元数据部分 ============
        out.write('  <!-- 项目元数据 -->\n')
        out.write('  <metadata>\n')
        out.write(f'    <name>{xml_escape(root.name)}</name>\n')
        out.write(f'    <path>{xml_escape(str(root))}</path>\n')
        out.write(f'    <export_time>{datetime.now().isoformat()}</export_time>\n')
        out.write(f'    <total_files>{len(code_files)}</total_files>\n')
        out.write(f'    <total_size_bytes>{total_size}</total_size_bytes>\n')
        out.write(f'    <total_size_mb>{total_size / (1024 * 1024):.2f}</total_size_mb>\n')
        out.write('    <file_types>\n')
        
        for ext, count in sorted(stats.items(), key=lambda x: (-x[1], x[0])):
            out.write(f'      <type extension="{xml_escape(ext)}" count="{count}"/>\n')
        
        out.write('    </file_types>\n')
        out.write('  </metadata>\n\n')
        
        # ============ 目录结构部分 ============
        out.write('  <!-- 目录结构 -->\n')
        out.write('  <directory_structure>\n')
        out.write('    <tree><![CDATA[\n')
        out.write(f'{root.name}/\n')
        out.write('\n'.join(tree_lines))
        out.write('\n    ]]></tree>\n')
        out.write('  </directory_structure>\n\n')
        
        # ============ 文件内容部分 ============
        out.write('  <!-- 源代码文件 -->\n')
        out.write('  <files>\n')
        
        print(f"\n[3/3] 读取文件内容...")
        
        for index, file_path in enumerate(code_files, 1):
            relative_path = file_path.relative_to(root)
            print(f"  [{index}/{len(code_files)}] {relative_path}")
            
            # 获取文件信息
            file_info = get_file_info(file_path, root)
            if not file_info:
                print(f"    ⚠ 无法获取文件信息")
                continue
            
            # 读取文件内容
            content, encoding, error = read_file_content(file_path)
            
            # 写入文件节点
            out.write(f'\n    <file index="{index}">\n')
            out.write(f'      <path>{xml_escape(file_info["path"])}</path>\n')
            out.write(f'      <name>{xml_escape(file_info["name"])}</name>\n')
            out.write(f'      <extension>{xml_escape(file_info["extension"])}</extension>\n')
            out.write(f'      <size_bytes>{file_info["size"]}</size_bytes>\n')
            out.write(f'      <modified>{file_info["modified"]}</modified>\n')
            
            if error:
                out.write(f'      <error>{xml_escape(error)}</error>\n')
                out.write('      <content/>\n')
                print(f"    ⚠ 读取失败: {error}")
            else:
                out.write(f'      <encoding>{encoding}</encoding>\n')
                out.write(f'      <lines>{len(content.splitlines())}</lines>\n')
                out.write('      <content><![CDATA[\n')
                out.write(content)
                if not content.endswith('\n'):
                    out.write('\n')
                out.write(']]></content>\n')
            
            out.write('    </file>\n')
        
        out.write('\n  </files>\n\n')
        out.write('</project>\n')
    
    # 输出统计信息
    print(f"\n{'=' * 60}")
    print(f"✓ 导出完成!")
    print(f"{'=' * 60}")
    print(f"输出文件: {output_path}")
    print(f"文件大小: {output_path.stat().st_size / 1024:.2f} KB")
    print(f"总计文件: {len(code_files)} 个")
    print(f"代码总量: {total_size / 1024:.2f} KB")
    print(f"\n文件类型统计:")
    for ext, count in sorted(stats.items(), key=lambda x: (-x[1], x[0]))[:10]:
        print(f"  {ext:15s} : {count:3d} 个")
    print(f"{'=' * 60}\n")


if __name__ == "__main__":
    # 导出当前目录
    export_project_xml()
    
    # 或指定其他目录
    # export_project_xml(root_path="/path/to/your/project")
    
    # 或指定输出文件名
    # export_project_xml(output_file="my_project.xml")
==================== END OF FILE: packer.py ====================

==================== START OF FILE: models/simple_cnn.py ====================
"""
简单的 CNN 模型（用于 CIFAR-10）

架构：
Conv(3->32) -> ReLU -> MaxPool -> Conv(32->64) -> ReLU -> MaxPool -> Flatten -> Linear(64*8*8->512) -> ReLU -> Linear(512->10)
"""
import numpy as np
from nn.module import Module, Parameter
from nn.conv import Conv2D, MaxPool2D
from nn.linear import Linear
from nn.activations import ReLU
from basic_operator import Value

class SimpleCNN(Module):
    """
    简单的卷积神经网络
    
    输入：[batch, 3, 32, 32] (CIFAR-10 图像)
    输出：[batch, 10] (10 个类别的 logits)
    """
    def __init__(self, num_classes=10):
        super().__init__()
        
        # 第一层卷积块：3 -> 32 channels
        self.conv1 = Conv2D(in_channels=3, out_channels=32, 
                           kernel_size=3, stride=1, padding=1)
        self.relu1 = ReLU()
        self.pool1 = MaxPool2D(kernel_size=2, stride=2)  # 32x32 -> 16x16
        
        # 第二层卷积块：32 -> 64 channels
        self.conv2 = Conv2D(in_channels=32, out_channels=64,
                           kernel_size=3, stride=1, padding=1)
        self.relu2 = ReLU()
        self.pool2 = MaxPool2D(kernel_size=2, stride=2)  # 16x16 -> 8x8
        
        # 全连接层
        self.fc1 = Linear(in_features=64 * 8 * 8, out_features=512)
        self.relu3 = ReLU()
        self.fc2 = Linear(in_features=512, out_features=num_classes)
    
    def forward(self, x: Value) -> Value:
        """
        前向传播
        
        Args:
            x: [batch, 3, 32, 32]
        
        Returns:
            logits: [batch, num_classes]
        """
        # 第一个卷积块
        x = self.conv1(x)       # [batch, 32, 32, 32]
        x = self.relu1(x)
        x = self.pool1(x)       # [batch, 32, 16, 16]
        
        # 第二个卷积块
        x = self.conv2(x)       # [batch, 64, 16, 16]
        x = self.relu2(x)
        x = self.pool2(x)       # [batch, 64, 8, 8]
        
        # Flatten
        x_data = x.realize_cached_data()
        batch_size = x_data.shape[0]
        flattened = x_data.reshape(batch_size, -1)  # [batch, 64*8*8]
        x = Value.make_const(flattened, requires_grad=x.requires_grad)
        x._init(x.op, x.inputs, cached_data=flattened, requires_grad=x.requires_grad)
        
        # 全连接层
        x = self.fc1(x)         # [batch, 512]
        x = self.relu3(x)
        x = self.fc2(x)         # [batch, 10]
        
        return x

# 简化版（更小的模型，训练更快）
class TinyCNN(Module):
    """
    更小的 CNN（用于快速测试）
    
    Conv(3->16) -> ReLU -> MaxPool -> Conv(16->32) -> ReLU -> MaxPool -> Linear(32*8*8->10)
    """
    def __init__(self, num_classes=10):
        super().__init__()
        
        self.conv1 = Conv2D(3, 16, kernel_size=3, stride=1, padding=1)
        self.relu1 = ReLU()
        self.pool1 = MaxPool2D(kernel_size=2, stride=2)
        
        self.conv2 = Conv2D(16, 32, kernel_size=3, stride=1, padding=1)
        self.relu2 = ReLU()
        self.pool2 = MaxPool2D(kernel_size=2, stride=2)
        
        self.fc = Linear(32 * 8 * 8, num_classes)
    
    def forward(self, x: Value) -> Value:
        x = self.conv1(x)
        x = self.relu1(x)
        x = self.pool1(x)
        
        x = self.conv2(x)
        x = self.relu2(x)
        x = self.pool2(x)
        
        # Flatten
        x_data = x.realize_cached_data()
        batch_size = x_data.shape[0]
        flattened = x_data.reshape(batch_size, -1)
        x = Value.make_const(flattened, requires_grad=x.requires_grad)
        x._init(x.op, x.inputs, cached_data=flattened, requires_grad=x.requires_grad)
        
        x = self.fc(x)
        return x
==================== END OF FILE: models/simple_cnn.py ====================

==================== START OF FILE: python/utils.py ====================
import math
from device import cpu
from task1_operators import Tensor

def rand(*shape, low=0.0, high=1.0, device=None, dtype="float32", requires_grad=False):
    """Generate random numbers uniform between low and high"""
    device = cpu() if device is None else device
    array = device.rand(*shape) * (high - low) + low
    return Tensor(array, device=device, dtype=dtype, requires_grad=requires_grad)


def randn(*shape, mean=0.0, std=1.0, device=None, dtype="float32", requires_grad=False):
    """Generate random normal with specified mean and std deviation"""
    device = cpu() if device is None else device
    array = device.randn(*shape) * std + mean
    return Tensor(array, device=device, dtype=dtype, requires_grad=requires_grad)


def constant(*shape, c=1.0, device=None, dtype="float32", requires_grad=False):
    """Generate constant Tensor"""
    device = cpu() if device is None else device
    array = device.ones(*shape, dtype=dtype) * c  # note: can change dtype
    return Tensor(array, device=device, dtype=dtype, requires_grad=requires_grad)


def ones(*shape, device=None, dtype="float32", requires_grad=False):
    """Generate all-ones Tensor"""
    return constant(
        *shape, c=1.0, device=device, dtype=dtype, requires_grad=requires_grad
    )


def zeros(*shape, device=None, dtype="float32", requires_grad=False):
    """Generate all-zeros Tensor"""
    return constant(
        *shape, c=0.0, device=device, dtype=dtype, requires_grad=requires_grad
    )


def randb(*shape, p=0.5, device=None, dtype="bool", requires_grad=False):
    """Generate binary random Tensor"""
    device = cpu() if device is None else device
    array = device.rand(*shape) <= p
    return Tensor(array, device=device, dtype=dtype, requires_grad=requires_grad)


def one_hot(n, i, device=None, dtype="float32", requires_grad=False):
    """Generate one-hot encoding Tensor"""
    device = cpu() if device is None else device
    return Tensor(
        device.one_hot(n, i.numpy(), dtype=dtype),
        device=device,
        requires_grad=requires_grad,
    )


def zeros_like(array, *, device=None, requires_grad=False):
    device = device if device else array.device
    return zeros(
        *array.shape, dtype=array.dtype, device=device, requires_grad=requires_grad
    )


def ones_like(array, *, device=None, requires_grad=False):
    device = device if device else array.device
    return ones(
        *array.shape, dtype=array.dtype, device=device, requires_grad=requires_grad
    )

==================== END OF FILE: python/utils.py ====================

==================== START OF FILE: python/core/device.py ====================
class Device:
    def __init__(self, device_type, device_id=0):
        self.device_type = device_type  # "cpu" or "cuda"
        self.device_id = device_id
        
    @staticmethod
    def cpu():
        return Device("cpu", 0)
    
    @staticmethod
    def cuda(device_id=0):
        return Device("cuda", device_id)
    
    def __repr__(self):
        if self.device_type == "cpu":
            return "cpu()"
        return f"cuda({self.device_id})"

cpu = Device.cpu()
cuda = Device.cuda
==================== END OF FILE: python/core/device.py ====================

==================== START OF FILE: python/core/basic_operator.py ====================
from typing import List, Optional
import numpy as np
import my_deep_lib

class Value:
    """基础 Value 类（计算图节点）"""
    def __init__(self):
        self.cached_data = None
        self.requires_grad = True
        self.grad = None
        self.op = None
        self.inputs = []
    
    def _init(self, op, inputs, *, cached_data=None, requires_grad=True):
        self.op = op
        self.inputs = inputs
        self.cached_data = cached_data
        self.requires_grad = requires_grad
    
    def realize_cached_data(self):
        return self.cached_data

    def backward(self, out_grad=None):
        # 1. 初始化梯度
        if out_grad is None:
            if isinstance(self.cached_data, my_deep_lib.Tensor):
                self.grad = my_deep_lib.Tensor(self.cached_data.shape(), self.cached_data.device())
                self.grad.fill(1.0)
            else:
                self.grad = np.ones_like(self.cached_data, dtype=np.float32)
        else:
            self.grad = out_grad

        # 2. 拓扑排序
        topo_order = []
        visited = set()
        
        def build_topo(node):
            if node not in visited:
                visited.add(node)
                for child in node.inputs:
                    build_topo(child)
                topo_order.append(node)
        
        build_topo(self)
        
        # 3. 反向传播
        for node in reversed(topo_order):
            if node.grad is None: continue
            if node.op is None: continue 
            
            grads = node.op.gradient(as_value(node.grad), node)
            
            for i, input_node in enumerate(node.inputs):
                if not input_node.requires_grad: continue
                
                # === 修复: 处理梯度为 None 的情况 (比如 Loss 对 Labels 的梯度) ===
                if i >= len(grads) or grads[i] is None:
                    continue

                g = grads[i].realize_cached_data()
                
                if input_node.grad is None:
                    input_node.grad = g
                else:
                    if isinstance(input_node.grad, my_deep_lib.Tensor):
                        input_node.grad = my_deep_lib.elementwise_add(input_node.grad, g)
                    else:
                        input_node.grad = input_node.grad + g

def as_value(data):
    v = Value()
    v.cached_data = data
    return v

class Op:
    def __call__(self, *args):
        inputs = [arg for arg in args]
        raw_inputs = [x.realize_cached_data() for x in inputs]
        output_data = self.compute(*raw_inputs)
        requires_grad = any(x.requires_grad for x in inputs)
        output_val = Value()
        output_val._init(self, inputs, cached_data=output_data, requires_grad=requires_grad)
        return output_val
    
    def compute(self, *args):
        raise NotImplementedError
    
    def gradient(self, out_grad, node):
        raise NotImplementedError
==================== END OF FILE: python/core/basic_operator.py ====================

==================== START OF FILE: python/nn/__init__.py ====================
from .module import Module, Parameter
from .conv import Conv2D
from .linear import Linear
from .activations import ReLU
from .pooling import MaxPool2D
from .loss import cross_entropy_loss
==================== END OF FILE: python/nn/__init__.py ====================

==================== START OF FILE: python/nn/pooling.py ====================
import my_deep_lib as cuda
from .module import Module
from basic_operator import Op, Value

class MaxPool2DOp(Op):
    def __init__(self, kernel_size, stride):
        self.kernel_size = kernel_size
        self.stride = stride
        self.mask = None
    
    def compute(self, x):
        # C++ extension 现在的 maxpool_forward 返回 (output, mask)
        out, mask = cuda.maxpool_forward(x, self.kernel_size, self.stride)
        
        # 我们必须只返回 output 给计算图的下一层
        # mask 保存起来用于反向传播
        self.mask = mask 
        return out

    def gradient(self, out_grad, node):
        grad_out = out_grad.realize_cached_data()
        
        # C++ extension 现在的 maxpool_backward 接受 (grad_output, mask, k, s)
        # 注意这里传入的是 self.mask，而不是 self.x
        dx = cuda.maxpool_backward(grad_out, self.mask, self.kernel_size, self.stride)
        
        gx = Value()
        gx._init(None, [], cached_data=dx)
        return (gx,)

class MaxPool2D(Module):
    def __init__(self, kernel_size, stride=None):
        super().__init__()
        self.k = kernel_size
        self.s = stride if stride is not None else kernel_size
    
    def forward(self, x):
        return MaxPool2DOp(self.k, self.s)(x)
==================== END OF FILE: python/nn/pooling.py ====================

==================== START OF FILE: python/nn/linear.py ====================
import numpy as np
import my_deep_lib as cuda
from basic_operator import Op, Value, as_value
from .module import Module, Parameter

class LinearOp(Op):
    def compute(self, x, w, b):
        # === 临时强制 CPU fallback 以规避 C++ SGEMM/Memory Error ===
        # 如果输入是 C++ Tensor，转回 Numpy 计算
        if isinstance(x, cuda.Tensor):
            x_np = x.to_numpy()
            w_np = w.to_numpy()
            b_np = b.to_numpy()
            
            # Numpy MatMul: (Batch, In) @ (In, Out) + (Out)
            # 此时我们的权重 w 是 [In, Out] 形状
            out_np = x_np @ w_np + b_np
            
            # 结果转回 C++ Tensor (GPU)
            # 注意：新 Tensor 需要在正确的 device 上
            return cuda.Tensor.from_numpy(out_np.astype(np.float32), x.device())
            
        # 纯 CPU 模式
        return x @ w + b

    def gradient(self, out_grad, node):
        x, w, b = node.inputs
        
        grad_out = out_grad.realize_cached_data()
        x_data = x.realize_cached_data()
        w_data = w.realize_cached_data()
        
        # === 临时强制 CPU fallback ===
        if isinstance(grad_out, cuda.Tensor):
            # 将所有数据转回 CPU
            dout_np = grad_out.to_numpy()
            x_np = x_data.to_numpy()
            w_np = w_data.to_numpy()
            
            # Numpy Backward
            # Y = X @ W + B
            # dL/dX = dL/dY @ W.T
            # dL/dW = X.T @ dL/dY
            # dL/dB = sum(dL/dY, axis=0)
            
            dx_np = dout_np @ w_np.T
            dw_np = x_np.T @ dout_np
            db_np = np.sum(dout_np, axis=0)
            
            # 转回 GPU
            dev = grad_out.device()
            dx = cuda.Tensor.from_numpy(dx_np.astype(np.float32), dev)
            dw = cuda.Tensor.from_numpy(dw_np.astype(np.float32), dev)
            db = cuda.Tensor.from_numpy(db_np.astype(np.float32), dev)
            
        else:
             dx = grad_out @ w_data.T
             dw = x_data.T @ grad_out
             db = np.sum(grad_out, axis=0)
             
        return as_value(dx), as_value(dw), as_value(db)

class Linear(Module):
    def __init__(self, in_features, out_features, bias=True):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        
        # 使用 [In, Out] 形状，这最符合 Numpy 的 X @ W 习惯
        limit = np.sqrt(6 / (in_features + out_features))
        w_data = np.random.uniform(-limit, limit, (in_features, out_features)).astype(np.float32)
        
        if hasattr(cuda.Tensor, "from_numpy"):
             w_tensor = cuda.Tensor.from_numpy(w_data, cuda.Device.gpu(0))
             b_tensor = cuda.Tensor.from_numpy(np.zeros(out_features, dtype=np.float32), cuda.Device.gpu(0))
             self.weight = Parameter(w_tensor)
             self.bias = Parameter(b_tensor)
        else:
             self.weight = Parameter(w_data)
             self.bias = Parameter(np.zeros(out_features, dtype=np.float32))
        
    def forward(self, x):
        return LinearOp()(x, self.weight, self.bias)
==================== END OF FILE: python/nn/linear.py ====================

==================== START OF FILE: python/nn/module.py ====================
import numpy as np
from basic_operator import Value
import my_deep_lib  # 引入库以进行类型检查

class Parameter(Value):
    def __init__(self, data):
        super().__init__()
        
        # 如果传入的已经是 Tensor 对象 (例如 C++ GPU Tensor)，直接使用
        if isinstance(data, my_deep_lib.Tensor):
            self.cached_data = data
        else:
            # 否则假设是 Numpy 兼容的数据，转换为 float32 数组
            self.cached_data = np.array(data, dtype=np.float32)
            
        self.requires_grad = True

class Module:
    def __init__(self):
        self._modules = {}
        self._params = {}
    
    def __setattr__(self, name, value):
        if isinstance(value, Parameter):
            self._params[name] = value
        elif isinstance(value, Module):
            self._modules[name] = value
        super().__setattr__(name, value)
    
    def parameters(self):
        params = []
        for p in self._params.values():
            params.append(p)
        for m in self._modules.values():
            params.extend(m.parameters())
        return params
    
    def train(self): 
        for m in self._modules.values():
            m.train()
            
    def eval(self): 
        for m in self._modules.values():
            m.eval()
            
    def __call__(self, *args):
        return self.forward(*args)
        
    def forward(self, *args):
        raise NotImplementedError
==================== END OF FILE: python/nn/module.py ====================

==================== START OF FILE: python/nn/conv.py ====================
import numpy as np
import my_deep_lib as cuda
from .module import Module, Parameter
from basic_operator import Op, Value, as_value

def im2col(x, kernel_size, stride, padding):
    # x: [N, C, H, W]
    N, C, H, W = x.shape
    out_h = (H + 2 * padding - kernel_size) // stride + 1
    out_w = (W + 2 * padding - kernel_size) // stride + 1
    
    # Pad
    x_padded = np.pad(x, ((0, 0), (0, 0), (padding, padding), (padding, padding)), mode='constant')
    
    col = np.zeros((N, C, kernel_size, kernel_size, out_h, out_w), dtype=np.float32)
    
    for y in range(kernel_size):
        for x_k in range(kernel_size):
            y_max = y + stride * out_h
            x_max = x_k + stride * out_w
            col[:, :, y, x_k, :, :] = x_padded[:, :, y:y_max:stride, x_k:x_max:stride]

    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N * out_h * out_w, -1)
    return col

class Conv2DOp(Op):
    def __init__(self, stride, padding, kernel_size):
        self.stride = stride
        self.padding = padding
        self.kernel_size = kernel_size
    
    def compute(self, x, w):
        # === 强制 CPU fallback 以规避 C++ SGEMM Error ===
        is_gpu_input = False
        device = None
        
        if isinstance(x, cuda.Tensor):
            is_gpu_input = True
            device = x.device()
            x_np = x.to_numpy()
            w_np = w.to_numpy()
        else:
            x_np = x
            w_np = w
            
        N, C, H, W = x_np.shape
        out_c, in_c, k, k = w_np.shape 
        
        self.x_shape = x_np.shape
        
        out_h = (H + 2 * self.padding - k) // self.stride + 1
        out_w = (W + 2 * self.padding - k) // self.stride + 1
        
        # 1. Im2Col
        self.x_col = im2col(x_np, k, self.stride, self.padding)
        # 2. Reshape weights
        w_col = w_np.reshape(out_c, -1)
        
        # 3. GEMM
        out = self.x_col @ w_col.T
        
        # 4. Reshape output
        out = out.reshape(N, out_h, out_w, out_c).transpose(0, 3, 1, 2)
        
        if is_gpu_input:
            return cuda.Tensor.from_numpy(out.astype(np.float32), device)
        return out
    
    def gradient(self, out_grad, node):
        x = node.inputs[0]
        w = node.inputs[1]
        
        grad_out = out_grad.realize_cached_data()
        
        is_gpu = False
        device = None
        if isinstance(grad_out, cuda.Tensor):
            is_gpu = True
            device = grad_out.device()
            dout = grad_out.to_numpy()
            w_np = w.realize_cached_data().to_numpy()
        else:
            dout = grad_out
            w_np = w.realize_cached_data()
            
        N, out_c, out_h, out_w = dout.shape
        # 获取正确维度信息
        out_c, in_c, k, k = w_np.shape
        
        # 1. Reshape dout
        dout_reshaped = dout.transpose(0, 2, 3, 1).reshape(-1, out_c)
        
        # 2. Gradient w.r.t weights (dw)
        dw = dout_reshaped.T @ self.x_col
        dw = dw.reshape(out_c, in_c, k, k)
        
        # 3. Gradient w.r.t input (dx) - Col2Im
        w_col = w_np.reshape(out_c, -1) # (Out, In*K*K)
        dx_col = dout_reshaped @ w_col # (N*H*W, In*K*K)
        
        # === 修复：这里之前用了未定义的 C，改为使用 in_c ===
        dx_padded = np.zeros((N, in_c, self.x_shape[2] + 2*self.padding, self.x_shape[3] + 2*self.padding), dtype=np.float32)
        
        # Col2Im Loop
        idx = 0
        for i in range(N):
            for h in range(out_h):
                for w_idx in range(out_w):
                    # 取出当前 patch 的梯度
                    patch_grad = dx_col[idx].reshape(in_c, k, k)
                    idx += 1
                    
                    h_start = h * self.stride
                    w_start = w_idx * self.stride
                    dx_padded[i, :, h_start:h_start+k, w_start:w_start+k] += patch_grad
        
        if self.padding > 0:
            dx = dx_padded[:, :, self.padding:-self.padding, self.padding:-self.padding]
        else:
            dx = dx_padded
        
        if is_gpu:
            return as_value(cuda.Tensor.from_numpy(dx, device)), as_value(cuda.Tensor.from_numpy(dw, device))
        return as_value(dx), as_value(dw)

class Conv2D(Module):
    def __init__(self, in_c, out_c, kernel_size, stride=1, padding=0):
        super().__init__()
        self.stride = stride
        self.padding = padding
        self.kernel_size = kernel_size
        
        scale = np.sqrt(2.0 / (in_c * kernel_size * kernel_size))
        w_data = (np.random.randn(out_c, in_c, kernel_size, kernel_size) * scale).astype(np.float32)
        
        if hasattr(cuda.Tensor, "from_numpy"):
            w_tensor = cuda.Tensor.from_numpy(w_data, cuda.Device.gpu(0))
            self.weight = Parameter(w_tensor)
        else:
            self.weight = Parameter(w_data)
    
    def forward(self, x):
        return Conv2DOp(self.stride, self.padding, self.kernel_size)(x, self.weight)
==================== END OF FILE: python/nn/conv.py ====================

==================== START OF FILE: python/nn/loss.py ====================
import numpy as np
import my_deep_lib as cuda
from basic_operator import Op, Value

class CrossEntropyOp(Op):
    def compute(self, logits, labels):
        self.logits = logits
        
        # 处理标签：如果是 numpy 数组，转为 Tensor
        if isinstance(labels, np.ndarray):
            # 标签通常是 float32 传入
            self.labels = cuda.Tensor.from_numpy(labels.astype(np.float32), cuda.Device.gpu(0))
        else:
            self.labels = labels
            
        # 返回 CPU float 用于打印 loss
        return cuda.cross_entropy_loss(self.logits, self.labels)
    
    def gradient(self, out_grad, node):
        # out_grad 通常是 1.0 (标量)
        # 我们计算 dLoss/dLogits
        
        d_logits = cuda.cross_entropy_backward(self.logits, self.labels)
        
        gv = Value()
        gv._init(None, [], cached_data=d_logits)
        return (gv, None)

def cross_entropy_loss(logits, labels):
    return CrossEntropyOp()(logits, labels)
==================== END OF FILE: python/nn/loss.py ====================

==================== START OF FILE: python/nn/activations.py ====================
import numpy as np
import my_deep_lib as cuda
from .module import Module
from basic_operator import Op, Value

class ReLUOp(Op):
    def compute(self, x):
        # x 是 Tensor
        self.x = x # 保存 input 用于反向传播
        
        # 修正：直接调用，C++ 会返回新的 Tensor
        return cuda.relu_forward(x)
    
    def gradient(self, out_grad, node):
        grad_out = out_grad.realize_cached_data()
        
        # 确保梯度是 Tensor
        if isinstance(grad_out, np.ndarray):
             # 假设在 GPU 0
            grad_out = cuda.Tensor.from_numpy(grad_out, cuda.Device.gpu(0))
            
        # 调用 C++ backward
        # 根据您的 pybind 定义，relu_backward 接受 (grad_output, input) 并返回 grad_input
        grad_x = cuda.relu_backward(grad_out, self.x)
        
        gx = Value()
        gx._init(None, [], cached_data=grad_x)
        return (gx,)

class ReLU(Module):
    def forward(self, x):
        return ReLUOp()(x)
==================== END OF FILE: python/nn/activations.py ====================

==================== START OF FILE: python/optim/adam.py ====================
"""
Adam 优化器
"""
import numpy as np

class Adam:
    """
    Adam 优化器
    """
    def __init__(self, params, lr=0.001, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.0):
        self.params = list(params)
        self.lr = lr
        self.beta1, self.beta2 = betas
        self.eps = eps
        self.weight_decay = weight_decay
        
        self.m = {}
        self.v = {}
        self.t = 0
        
        for i, p in enumerate(self.params):
            shape = p.realize_cached_data().shape
            self.m[i] = np.zeros(shape, dtype=np.float32)
            self.v[i] = np.zeros(shape, dtype=np.float32)
    
    def step(self):
        self.t += 1
        
        for i, param in enumerate(self.params):
            if param.grad is None:
                continue
            
            grad = param.grad.realize_cached_data()
            data = param.realize_cached_data()
            
            if self.weight_decay != 0:
                grad = grad + self.weight_decay * data
            
            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad
            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (grad ** 2)
            
            m_hat = self.m[i] / (1 - self.beta1 ** self.t)
            v_hat = self.v[i] / (1 - self.beta2 ** self.t)
            
            param.cached_data = (data - self.lr * m_hat / (np.sqrt(v_hat) + self.eps)).astype(np.float32)
    
    def zero_grad(self):
        for param in self.params:
            param.grad = None
==================== END OF FILE: python/optim/adam.py ====================

==================== START OF FILE: python/optim/__init__.py ====================
from .sgd import SGD
from .adam import Adam

__all__ = ['SGD', 'Adam']
==================== END OF FILE: python/optim/__init__.py ====================

==================== START OF FILE: python/optim/sgd.py ====================
from basic_operator import Value
import numpy as np
import my_deep_lib as cuda

class SGD:
    def __init__(self, params, lr=0.01, momentum=0.0):
        self.params = params
        self.lr = lr
        self.momentum = momentum
        self.velocities = [None] * len(params) # 动量缓存

    def zero_grad(self):
        for p in self.params:
            p.grad = None

    def step(self):
        for i, param in enumerate(self.params):
            if param.grad is None:
                continue
            
            # 1. 获取梯度数据
            # param.grad 可能是 Value 对象，也可能是直接的 Tensor/Numpy
            if hasattr(param.grad, "realize_cached_data"):
                grad = param.grad.realize_cached_data()
            else:
                grad = param.grad # 已经是 Tensor 或 Numpy
            
            # 使用 param.cached_data (Tensor 或 Numpy)
            p_data = param.cached_data
            
            # 2. 如果是 C++ Tensor
            if isinstance(p_data, cuda.Tensor):
                # 简单的 SGD update: param = param - lr * grad
                # 这里我们还没有实现 C++ 端的 SGD optimizer kernel
                # 所以我们只能用 elementwise_add 和 标量乘法
                # 假设 elementwise_add(a, b) -> a + b
                # p = p - lr * g  => p = p + (-lr * g)
                
                # 为了简便，目前我们没有 C++ 的 scale 函数，这是个大问题！
                # 临时方案：将 Tensor 转回 CPU 更新，再转回 GPU (极慢，但能跑通)
                # 或者，假设你以后会实现 optimizer_step kernel
                
                # === 临时 Hack: CPU update ===
                cpu_p = p_data.to_numpy()
                
                if isinstance(grad, cuda.Tensor):
                    cpu_g = grad.to_numpy()
                else:
                    cpu_g = grad
                    
                # 动量更新
                if self.momentum > 0:
                    if self.velocities[i] is None:
                        self.velocities[i] = np.zeros_like(cpu_p)
                    v = self.velocities[i]
                    v[:] = self.momentum * v + cpu_g
                    cpu_p -= self.lr * v
                else:
                    cpu_p -= self.lr * cpu_g
                
                # 写回 GPU
                # 注意：这里需要 copy 回去，而不是新建 Tensor 替换，
                # 因为 param.cached_data 的引用可能被其他地方持有
                # p_data.copy_from(cpu_p) # 如果有这个接口
                # 没有的话，重新 from_numpy 并覆盖
                new_tensor = cuda.Tensor.from_numpy(cpu_p, p_data.device())
                # 我们假设这是一个浅拷贝或者移动，或者我们直接修改 param 引用
                param.cached_data = new_tensor
                
            else:
                # Numpy 模式
                if self.momentum > 0:
                    if self.velocities[i] is None:
                        self.velocities[i] = np.zeros_like(p_data)
                    v = self.velocities[i]
                    v[:] = self.momentum * v + grad
                    p_data -= self.lr * v
                else:
                    p_data -= self.lr * grad
==================== END OF FILE: python/optim/sgd.py ====================

==================== START OF FILE: python/cuda_ops/__init__.py ====================
"""
CUDA 算子的统一导出
"""
from .conv import conv2d
from .pooling import maxpool2d
from .linear import linear
from .activations import relu, softmax, sigmoid

__all__ = [
    'conv2d',
    'maxpool2d',
    'linear',
    'relu',
    'softmax',
    'sigmoid',
]
==================== END OF FILE: python/cuda_ops/__init__.py ====================

==================== START OF FILE: python/cuda_ops/pooling.py ====================
"""
CUDA MaxPool2D 算子的 Python 包装
"""
import numpy as np
from typing import Tuple
import my_deep_lib as cuda_lib
from basic_operator import Op, Value

class MaxPool2DOp(Op):
    def __init__(self, kernel_size, stride=None, padding=0):
        self.kernel_size = kernel_size
        self.stride = stride if stride is not None else kernel_size
        self.padding = padding
        self.mask = None  # 存储 forward 时的 mask，用于 backward
    
    def compute(self, input_data: np.ndarray) -> np.ndarray:
        """
        input_data: [batch, channels, height, width]
        """
        batch, channels, in_h, in_w = input_data.shape
        
        # 计算输出尺寸
        out_h = (in_h + 2 * self.padding - self.kernel_size) // self.stride + 1
        out_w = (in_w + 2 * self.padding - self.kernel_size) // self.stride + 1
        
        # 转换为 CUDA Tensor
        cuda_input = cuda_lib.Tensor.from_numpy(input_data, cuda_lib.Device.gpu(0))
        cuda_output = cuda_lib.Tensor([batch, channels, out_h, out_w], cuda_lib.Device.gpu(0))
        cuda_mask = cuda_lib.Tensor([batch, channels, out_h, out_w], cuda_lib.Device.gpu(0))
        
        # 调用 CUDA kernel
        cuda_lib.maxpool_forward(cuda_input, cuda_output, cuda_mask,
                                self.kernel_size, self.stride, self.padding)
        
        # 保存 mask 用于 backward（转为 NumPy 保存）
        self.mask = cuda_mask.to_numpy()
        
        return cuda_output.to_numpy()
    
    def gradient(self, out_grad: Value, node: Value) -> Tuple[Value]:
        """
        MaxPool 的梯度传播
        """
        input_node = node.inputs[0]
        input_data = input_node.realize_cached_data()
        grad_output_data = out_grad.realize_cached_data()
        
        # 转换为 CUDA Tensor
        cuda_grad_output = cuda_lib.Tensor.from_numpy(grad_output_data, cuda_lib.Device.gpu(0))
        cuda_mask = cuda_lib.Tensor.from_numpy(self.mask, cuda_lib.Device.gpu(0))
        cuda_grad_input = cuda_lib.Tensor(list(input_data.shape), cuda_lib.Device.gpu(0))
        
        # 调用 CUDA backward kernel
        cuda_lib.maxpool_backward(cuda_grad_output, cuda_mask, cuda_grad_input,
                                 self.kernel_size, self.stride, self.padding)
        
        grad_input_np = cuda_grad_input.to_numpy()
        grad_input = Value.make_const(grad_input_np, requires_grad=False)
        
        return (grad_input,)

def maxpool2d(input: Value, kernel_size, stride=None, padding=0) -> Value:
    """
    MaxPool2D 操作
    
    Args:
        input: [batch, channels, H, W]
        kernel_size: 池化窗口大小
        stride: 步长（默认等于 kernel_size）
        padding: 填充
    
    Returns:
        output: [batch, channels, H_out, W_out]
    """
    return MaxPool2DOp(kernel_size, stride, padding)(input)
==================== END OF FILE: python/cuda_ops/pooling.py ====================

==================== START OF FILE: python/cuda_ops/linear.py ====================
import numpy as np
import my_deep_lib as cuda
from .module import Module, Parameter
from basic_operator import Op, Value

class LinearOp(Op):
    def __init__(self, weight, bias):
        self.weight = weight
        self.bias = bias
    
    def compute(self, x):
        # 1. 自动 Flatten 处理
        # 如果输入是 (N, C, H, W)，需要变成 (N, C*H*W)
        shape = x.shape()
        if len(shape) > 2:
            batch = shape[0]
            numel = x.numel() // batch
            # 使用我们在 pybind 里新加的 reshape
            self.x = x.reshape([batch, numel])
        else:
            self.x = x

        # 2. 准备权重
        w = self.weight.realize_cached_data()
        if isinstance(w, np.ndarray):
            self.w = cuda.Tensor.from_numpy(w, cuda.Device.gpu(0))
        else:
            self.w = w
            
        b = self.bias.realize_cached_data()
        if isinstance(b, np.ndarray):
            self.b = cuda.Tensor.from_numpy(b, cuda.Device.gpu(0))
        else:
            self.b = b
            
        return cuda.linear_forward(self.x, self.w, self.b)
    
    def gradient(self, out_grad, node):
        grad_out = out_grad.realize_cached_data()
        if isinstance(grad_out, np.ndarray):
            grad_out = cuda.Tensor.from_numpy(grad_out, cuda.Device.gpu(0))
            
        dx, dw, db = cuda.linear_backward(grad_out, self.x, self.w)
        
        # 处理输入的梯度 (dx)
        # 如果之前做了 reshape，这里其实应该把 shape 变回去，
        # 但对于全连接层的反向传播，通常只需要传回 flat 的梯度即可，或者上一层是 MaxPoolBackward 会自己处理形状
        gx = Value()
        gx._init(None, [], cached_data=dx)
        
        # 处理参数梯度
        if self.weight.grad is None:
            self.weight.grad = Value()
            self.weight.grad._init(None, [], cached_data=dw)
        else:
            self.weight.grad.cached_data = dw
            
        if self.bias.grad is None:
            self.bias.grad = Value()
            self.bias.grad._init(None, [], cached_data=db)
        else:
            self.bias.grad.cached_data = db
            
        return (gx,)

class Linear(Module):
    def __init__(self, in_f, out_f):
        super().__init__()
        # Kaiming init
        self.weight = Parameter(np.random.randn(out_f, in_f).astype(np.float32) * np.sqrt(2.0/in_f))
        self.bias = Parameter(np.zeros(out_f, dtype=np.float32))
    
    def forward(self, x):
        return LinearOp(self.weight, self.bias)(x)
==================== END OF FILE: python/cuda_ops/linear.py ====================

==================== START OF FILE: python/cuda_ops/conv.py ====================
import numpy as np
import my_deep_lib as cuda
from .module import Module, Parameter
from basic_operator import Op, Value

class Conv2DOp(Op):
    def __init__(self, weight, stride, padding):
        self.weight = weight
        self.stride = stride
        self.padding = padding
    
    def compute(self, x):
        if isinstance(x, np.ndarray):
            self.x = cuda.Tensor.from_numpy(x, cuda.Device.gpu(0))
        else:
            self.x = x

        w_data = self.weight.realize_cached_data()
        if isinstance(w_data, np.ndarray):
            self.w = cuda.Tensor.from_numpy(w_data, cuda.Device.gpu(0))
        else:
            self.w = w_data

        return cuda.conv2d_forward(self.x, self.w, self.stride, self.padding)
    
    def gradient(self, out_grad, node):
        grad_out = out_grad.realize_cached_data()
        if isinstance(grad_out, np.ndarray):
            grad_out = cuda.Tensor.from_numpy(grad_out, cuda.Device.gpu(0))
            
        # 修正：移除所有手动计算的 batch, h, w 等参数
        # 只传：input, grad_output, weight, stride, padding
        # C++ 端会自动从 Tensor.shape() 获取维度
        dx, dw = cuda.conv2d_backward(self.x, grad_out, self.w, self.stride, self.padding)
        
        gx = Value()
        gx._init(None, [], cached_data=dx)
        
        if self.weight.grad is None:
            self.weight.grad = Value()
            self.weight.grad._init(None, [], cached_data=dw)
        else:
            # 简单覆盖 (实际应累加，但暂且这样)
            self.weight.grad.cached_data = dw 

        return (gx,)

class Conv2D(Module):
    def __init__(self, in_c, out_c, kernel_size, stride=1, padding=0):
        super().__init__()
        self.stride = stride
        self.padding = padding
        scale = np.sqrt(2.0 / (in_c * kernel_size * kernel_size))
        self.weight = Parameter((np.random.randn(out_c, in_c, kernel_size, kernel_size) * scale).astype(np.float32))
    
    def forward(self, x):
        return Conv2DOp(self.weight, self.stride, self.padding)(x)
==================== END OF FILE: python/cuda_ops/conv.py ====================

==================== START OF FILE: python/cuda_ops/activations.py ====================
"""
激活函数的 CUDA 算子包装
"""
import numpy as np
from typing import Tuple
import my_deep_lib as cuda_lib
from basic_operator import Op, Value

class ReLUOp(Op):
    """ReLU 激活函数"""
    
    def compute(self, input_data: np.ndarray) -> np.ndarray:
        """
        前向传播：y = max(0, x)
        """
        # 转换为 CUDA Tensor
        cuda_input = cuda_lib.Tensor.from_numpy(input_data, cuda_lib.Device.gpu(0))
        cuda_output = cuda_lib.Tensor(list(input_data.shape), cuda_lib.Device.gpu(0))
        
        # 调用 CUDA kernel
        cuda_lib.relu_forward(cuda_input, cuda_output)
        
        return cuda_output.to_numpy()
    
    def gradient(self, out_grad: Value, node: Value) -> Tuple[Value]:
        """
        反向传播：grad_input = grad_output * (input > 0)
        """
        input_node = node.inputs[0]
        input_data = input_node.realize_cached_data()
        grad_output_data = out_grad.realize_cached_data()
        
        # 转换为 CUDA Tensor
        cuda_input = cuda_lib.Tensor.from_numpy(input_data, cuda_lib.Device.gpu(0))
        cuda_grad_output = cuda_lib.Tensor.from_numpy(grad_output_data, cuda_lib.Device.gpu(0))
        cuda_grad_input = cuda_lib.Tensor(list(input_data.shape), cuda_lib.Device.gpu(0))
        
        # 调用 CUDA backward kernel
        cuda_lib.relu_backward(cuda_input, cuda_grad_output, cuda_grad_input)
        
        grad_input_np = cuda_grad_input.to_numpy()
        grad_input = Value.make_const(grad_input_np, requires_grad=False)
        
        return (grad_input,)

class SoftmaxOp(Op):
    """Softmax 激活函数（通常用于输出层）"""
    
    def compute(self, input_data: np.ndarray) -> np.ndarray:
        """
        前向传播：softmax(x) = exp(x) / sum(exp(x))
        input_data: [batch, num_classes]
        """
        # 转换为 CUDA Tensor
        cuda_input = cuda_lib.Tensor.from_numpy(input_data, cuda_lib.Device.gpu(0))
        cuda_output = cuda_lib.Tensor(list(input_data.shape), cuda_lib.Device.gpu(0))
        
        # 调用 CUDA kernel
        cuda_lib.softmax_forward(cuda_input, cuda_output)
        
        return cuda_output.to_numpy()
    
    def gradient(self, out_grad: Value, node: Value) -> Tuple[Value]:
        """
        Softmax 的梯度比较复杂，但通常与 CrossEntropy 融合计算
        这里提供一个简化版本
        """
        # 获取 softmax 的输出
        output_data = node.realize_cached_data()
        grad_output_data = out_grad.realize_cached_data()
        
        # Softmax 梯度：grad_input = softmax * (grad_output - sum(grad_output * softmax))
        # 这里用 NumPy 实现（因为 CUDA kernel 里没有单独的 softmax backward）
        sum_term = np.sum(grad_output_data * output_data, axis=1, keepdims=True)
        grad_input_np = output_data * (grad_output_data - sum_term)
        
        grad_input = Value.make_const(grad_input_np, requires_grad=False)
        return (grad_input,)

class SigmoidOp(Op):
    """Sigmoid 激活函数"""
    
    def compute(self, input_data: np.ndarray) -> np.ndarray:
        """前向传播：y = 1 / (1 + exp(-x))"""
        cuda_input = cuda_lib.Tensor.from_numpy(input_data, cuda_lib.Device.gpu(0))
        cuda_output = cuda_lib.Tensor(list(input_data.shape), cuda_lib.Device.gpu(0))
        
        cuda_lib.sigmoid_forward(cuda_input, cuda_output)
        
        return cuda_output.to_numpy()
    
    def gradient(self, out_grad: Value, node: Value) -> Tuple[Value]:
        """反向传播：grad_input = grad_output * y * (1 - y)"""
        output_data = node.realize_cached_data()  # sigmoid 的输出
        grad_output_data = out_grad.realize_cached_data()
        
        cuda_output = cuda_lib.Tensor.from_numpy(output_data, cuda_lib.Device.gpu(0))
        cuda_grad_output = cuda_lib.Tensor.from_numpy(grad_output_data, cuda_lib.Device.gpu(0))
        cuda_grad_input = cuda_lib.Tensor(list(output_data.shape), cuda_lib.Device.gpu(0))
        
        cuda_lib.sigmoid_backward(cuda_output, cuda_grad_output, cuda_grad_input)
        
        grad_input_np = cuda_grad_input.to_numpy()
        grad_input = Value.make_const(grad_input_np, requires_grad=False)
        
        return (grad_input,)

# 高层接口
def relu(x: Value) -> Value:
    """ReLU 激活函数"""
    return ReLUOp()(x)

def softmax(x: Value) -> Value:
    """Softmax 激活函数"""
    return SoftmaxOp()(x)

def sigmoid(x: Value) -> Value:
    """Sigmoid 激活函数"""
    return SigmoidOp()(x)
==================== END OF FILE: python/cuda_ops/activations.py ====================

==================== START OF FILE: pybind/pybind_wrapper.cpp ====================
#include <pybind11/pybind11.h>
#include <pybind11/stl.h>   // 必须包含，用于 std::vector/pair 转换
#include <pybind11/numpy.h> 
#include <cuda_runtime.h>
#include <cublas_v2.h>
#include <cstring>
#include <vector>
#include <utility>          // std::move, std::pair
#include <tuple>            // std::tuple
#include <stdexcept>
#include <iostream>

#include "tensor.h"
#include "layers.h"
#include "conv_layer.h"  
#include "elementwise.h"

namespace py = pybind11;
using namespace std;

// 辅助函数：从 numpy array 创建 Tensor (保持不变)
Tensor from_numpy(py::array_t<float> array, Device device) {
    py::buffer_info buf = array.request();
    std::vector<int64_t> shape;
    for (auto s : buf.shape) shape.push_back(static_cast<int64_t>(s));
    Tensor t(shape, device);
    if (device.type == DeviceType::CPU) {
        std::memcpy(t.data(), buf.ptr, buf.size * sizeof(float));
    } else {
        cudaMemcpy(t.data(), buf.ptr, buf.size * sizeof(float), cudaMemcpyHostToDevice);
    }
    return t;
}

// 辅助函数：Tensor 转 Numpy (保持不变)
py::array_t<float> to_numpy(const Tensor& t) {
    std::vector<int64_t> shape = t.shape();
    std::vector<ssize_t> strides;
    ssize_t stride = sizeof(float);
    for (int i = shape.size() - 1; i >= 0; --i) {
        strides.insert(strides.begin(), stride);
        stride *= shape[i];
    }
    py::array_t<float> result(shape, strides);
    py::buffer_info buf = result.request();
    Tensor cpu_t = t.cpu(); 
    std::memcpy(buf.ptr, cpu_t.data(), cpu_t.numel() * sizeof(float));
    return result;
}

// 辅助函数：深度拷贝 (保持不变)
void safe_copy(const Tensor& src, Tensor& dst) {
    size_t nbytes = src.numel() * sizeof(float);
    if (src.device().type == DeviceType::GPU && dst.device().type == DeviceType::GPU) {
        cudaMemcpy(dst.data(), src.data(), nbytes, cudaMemcpyDeviceToDevice);
    } else {
        Tensor src_cpu = src.cpu();
        cudaMemcpy(dst.data(), src_cpu.data(), nbytes, cudaMemcpyHostToDevice);
    }
}

PYBIND11_MODULE(my_deep_lib, m) {
    m.doc() = "Deep Learning Library with CUDA Backend";

    // 1. Device
    py::class_<Device>(m, "Device")
        .def(py::init<DeviceType, int>(), py::arg("type"), py::arg("index")=0)
        .def_static("cpu", &Device::cpu)
        .def_static("gpu", &Device::gpu)
        .def_readonly("type", &Device::type)
        .def_readonly("index", &Device::index);

    // 2. DeviceType
    py::enum_<DeviceType>(m, "DeviceType")
        .value("CPU", DeviceType::CPU)
        .value("GPU", DeviceType::GPU)
        .export_values();

    // 3. Tensor
    py::class_<Tensor>(m, "Tensor")
        .def(py::init<const std::vector<int64_t>&, Device>())
        .def_static("from_numpy", &from_numpy)
        .def("to_numpy", &to_numpy)
        .def("fill", &Tensor::fill)
        .def_property_readonly("shape", &Tensor::shape)
        .def("numel", &Tensor::numel)
        .def("device", &Tensor::device)
        .def("reshape", [](Tensor& t, std::vector<int64_t> new_shape) {
            int64_t total_elements = t.numel();
            int64_t infer_idx = -1;
            int64_t new_count = 1;
            
            for(size_t i=0; i<new_shape.size(); ++i) {
                if (new_shape[i] == -1) {
                    if (infer_idx != -1) throw std::runtime_error("Only one dimension can be -1");
                    infer_idx = i;
                } else {
                    new_count *= new_shape[i];
                }
            }
            
            if (infer_idx != -1) {
                if (new_count == 0 || total_elements % new_count != 0) 
                    throw std::runtime_error("Invalid reshape dimensions");
                new_shape[infer_idx] = total_elements / new_count;
            } else {
                if (new_count != total_elements) 
                    throw std::runtime_error("Reshape size mismatch");
            }
        
            Tensor out(new_shape, t.device());
            safe_copy(t, out);
            return out;
        }, py::return_value_policy::move)
        .def("cpu", &Tensor::cpu, py::return_value_policy::move)
        .def("gpu", &Tensor::gpu, py::arg("index")=0, py::return_value_policy::move);

    // 4. Activations
    m.def("sigmoid_forward", [](const Tensor& input) {
        Tensor output(input.shape(), input.device());
        sigmoid_forward(input, output);
        return output;
    }, py::arg("input"), py::return_value_policy::move);

    m.def("sigmoid_backward", [](const Tensor& output, const Tensor& grad_output) {
        Tensor grad_input(output.shape(), output.device());
        sigmoid_backward(output, grad_output, grad_input);
        return grad_input;
    }, py::arg("output"), py::arg("grad_output"), py::return_value_policy::move);
    
    m.def("relu_forward", [](const Tensor& input) {
        Tensor output(input.shape(), input.device());
        relu_forward(input, output);
        return output;
    }, py::arg("input"), py::return_value_policy::move);
    
    m.def("relu_backward", [](const Tensor& grad_output, const Tensor& input) {
        Tensor grad_input(input.shape(), input.device());
        relu_backward(input, grad_output, grad_input);
        return grad_input;
    }, py::arg("grad_output"), py::arg("input"), py::return_value_policy::move);
    
    // 5. Linear
    m.def("linear_forward", [](const Tensor& input, const Tensor& weight, const Tensor& bias) {
        int batch = input.shape()[0];
        int out_features = weight.shape()[0];
        Tensor output({batch, out_features}, input.device());
        linear_forward(input, weight, bias, output); 
        return output;
    }, py::arg("input"), py::arg("weight"), py::arg("bias"), py::return_value_policy::move);

    // === 修复: 返回 std::tuple 而不是 py::make_tuple ===
    m.def("linear_backward", [](const Tensor& grad_output, const Tensor& input, const Tensor& weight) {
        int batch = input.shape()[0];
        int in_features = input.shape()[1];
        int out_features = weight.shape()[0];
        Tensor grad_input({batch, in_features}, input.device());
        Tensor grad_weight({out_features, in_features}, input.device());
        Tensor grad_bias({out_features}, input.device());
        
        linear_backward(input, weight, grad_output, grad_input, grad_weight, grad_bias);
        
        // 使用 std::make_tuple 配合 std::move
        return std::make_tuple(std::move(grad_input), std::move(grad_weight), std::move(grad_bias));
    }, py::arg("grad_output"), py::arg("input"), py::arg("weight"), py::return_value_policy::move);

    
    // 6. Conv2D
    m.def("conv2d_forward", [](const Tensor& input, const Tensor& weight, int stride, int padding) {
        int batch = input.shape()[0];
        int in_c = input.shape()[1];
        int h = input.shape()[2];
        int w = input.shape()[3];
        int out_c = weight.shape()[0];
        int ksize = weight.shape()[2];
        
        int h_out = (h + 2 * padding - ksize) / stride + 1;
        int w_out = (w + 2 * padding - ksize) / stride + 1;
        
        Tensor output({batch, out_c, h_out, w_out}, input.device());
        Tensor col_buffer({in_c * ksize * ksize, h_out * w_out}, input.device());
        
        cublasHandle_t handle;
        cublasCreate(&handle);
        forward_conv(const_cast<float*>(input.data()), 
                     output.data(), 
                     const_cast<float*>(weight.data()), 
                     col_buffer.data(), batch, in_c, out_c, h, w, ksize, stride, padding, handle);
        cublasDestroy(handle);
        return output;
    }, py::arg("input"), py::arg("weight"), py::arg("stride"), py::arg("padding"), py::return_value_policy::move);

    // === 修复: 返回 std::pair ===
    m.def("conv2d_backward", [](const Tensor& input, const Tensor& grad_output, const Tensor& weight, int stride, int padding) {
        int batch = input.shape()[0];
        int in_c = input.shape()[1];
        int h = input.shape()[2];
        int w = input.shape()[3];
        int out_c = weight.shape()[0];
        int ksize = weight.shape()[2];
        int h_out = grad_output.shape()[2];
        int w_out = grad_output.shape()[3];

        Tensor grad_input(input.shape(), input.device());
        Tensor grad_weight(weight.shape(), weight.device());
        Tensor col_buffer({in_c * ksize * ksize, h_out * w_out}, input.device());

        cublasHandle_t handle;
        cublasCreate(&handle);
        
        backward_conv(const_cast<float*>(input.data()), 
                      const_cast<float*>(grad_output.data()), 
                      const_cast<float*>(weight.data()),
                      col_buffer.data(), 
                      grad_input.data(), 
                      grad_weight.data(),
                      batch, in_c, out_c, h, w, ksize, stride, padding, handle);
                      
        cublasDestroy(handle);
        // 使用 std::make_pair 配合 std::move
        return std::make_pair(std::move(grad_input), std::move(grad_weight));
    }, py::arg("input"), py::arg("grad_output"), py::arg("weight"), py::arg("stride"), py::arg("padding"), py::return_value_policy::move);

    // 7. MaxPool
    // === 修复: 返回 std::pair ===
    m.def("maxpool_forward", [](const Tensor& input, int ksize, int stride) {
        int batch = input.shape()[0];
        int c = input.shape()[1];
        int h = input.shape()[2];
        int w = input.shape()[3];
        int h_out = (h - ksize) / stride + 1;
        int w_out = (w - ksize) / stride + 1;
        
        Tensor output({batch, c, h_out, w_out}, input.device());
        Tensor mask({batch, c, h_out, w_out}, input.device());
        
        maxpool_forward(input, output, mask, ksize, stride, 0);
        
        // 关键修复: 这里返回 std::pair，pybind 会将其转为 python tuple
        // 并且外层的 return_value_policy::move 能够作用于 pair 内的元素
        return std::make_pair(std::move(output), std::move(mask));
    }, py::arg("input"), py::arg("ksize"), py::arg("stride"), 
       py::return_value_policy::move);

    m.def("maxpool_backward", [](const Tensor& grad_output, const Tensor& mask, int ksize, int stride) {
        int batch = mask.shape()[0];
        int c = mask.shape()[1];
        int h_out = mask.shape()[2];
        int w_out = mask.shape()[3];
        
        int h_in = (h_out - 1) * stride + ksize;
        int w_in = (w_out - 1) * stride + ksize;
        
        Tensor grad_input({batch, c, h_in, w_in}, mask.device());

        maxpool_backward(grad_output, mask, grad_input, ksize, stride, 0);
        return grad_input;
    }, py::arg("grad_output"), py::arg("mask"), py::arg("ksize"), py::arg("stride"),
       py::return_value_policy::move); 
    
    // 8. Loss 
    m.def("cross_entropy_loss", [](const Tensor& logits, const Tensor& labels) {
        Tensor probs(logits.shape(), logits.device());
        softmax_forward(logits, probs); 
        return cross_entropy_loss(probs, labels); 
    });
    
    m.def("cross_entropy_backward", [](const Tensor& logits, const Tensor& labels) {
        Tensor probs(logits.shape(), logits.device());
        softmax_forward(logits, probs);
        
        Tensor grad_input(logits.shape(), logits.device());
        cross_entropy_backward(probs, labels, grad_input);
        return grad_input;
    }, py::arg("logits"), py::arg("labels"), py::return_value_policy::move);
    
    // 9. Elementwise
    m.def("elementwise_add", [](const Tensor& a, const Tensor& b) {
        Tensor c(a.shape(), a.device());
        // elementwise_add(a, b, c); // 假设实现存在
        return c;
    }, py::arg("a"), py::arg("b"), py::return_value_policy::move);
}
==================== END OF FILE: pybind/pybind_wrapper.cpp ====================

==================== START OF FILE: cuda_src/activations.h ====================
#pragma once
#include "tensor.h"

// ReLU
Tensor relu_forward(const Tensor& x);
Tensor relu_backward(const Tensor& x, const Tensor& grad_y); 

// Sigmoid
Tensor sigmoid_forward(const Tensor& x);
Tensor sigmoid_backward(const Tensor& y, const Tensor& grad_y); 
==================== END OF FILE: cuda_src/activations.h ====================

==================== START OF FILE: cuda_src/conv_layer.h ====================
#ifndef CONV_LAYER_H
#define CONV_LAYER_H

#include <cuda_runtime.h>
#include <cublas_v2.h>

// 前向传播
// input: [batch, in_c, h, w]
// output: [batch, out_c, h_out, w_out]
void forward_conv(float* d_input, float* d_output, float* d_weight, float* d_col_buffer,
                  int batch_size, int in_channels, int out_channels, 
                  int height, int width, int ksize, int stride, int padding,
                  cublasHandle_t handle);

// 反向传播
void backward_conv(float* d_input, float* d_grad_output, float* d_weight, 
                   float* d_col_buffer, float* d_grad_input, float* d_grad_weight,
                   int batch_size, int in_channels, int out_channels, 
                   int height, int width, int ksize, int stride, int padding,
                   cublasHandle_t handle);

#endif
==================== END OF FILE: cuda_src/conv_layer.h ====================

==================== START OF FILE: cuda_src/tensor.h ====================
#pragma once
#include <vector>
#include <memory>
#include <cstdint>
#include <stdexcept>

enum class DeviceType { CPU, GPU };

struct Device {
    DeviceType type;
    int index;

    Device() : type(DeviceType::CPU), index(0) {} 

    Device(DeviceType t, int i = 0) : type(t), index(i) {}
    static Device cpu() { return Device(DeviceType::CPU); }
    static Device gpu(int i = 0) { return Device(DeviceType::GPU, i); }
};

class Tensor {
public:
    Tensor(const std::vector<int64_t>& shape, Device device);
    ~Tensor();
    
    Tensor(Tensor&& other) noexcept;
    Tensor& operator=(Tensor&& other) noexcept;
    Tensor(const Tensor&) = delete;
    Tensor& operator=(const Tensor&) = delete;

    void fill(float v);
    Tensor cpu() const;
    Tensor gpu(int idx = 0) const;
    
    float* data();
    const float* data() const;
    
    const std::vector<int64_t>& shape() const { return shape_; }
    int64_t numel() const { return numel_; }
    const Device& device() const { return device_; }

private:
    void allocate();
    void deallocate();
    void copy_to(Tensor& dst) const;

    std::vector<int64_t> shape_;
    int64_t numel_;
    Device device_;
    void* storage_ = nullptr;
};
==================== END OF FILE: cuda_src/tensor.h ====================

==================== START OF FILE: cuda_src/activations.cu ====================
#include "activations.h"
#include <stdexcept>
#include <cmath>
#ifdef __CUDACC__
  #include <cuda_runtime.h>
  #define CUDA_CHECK(cmd) do { \
    cudaError_t e = (cmd); \
    if (e != cudaSuccess) throw std::runtime_error(cudaGetErrorString(e)); \
  } while(0)
#endif

//  CPU 实现 
static Tensor relu_forward_cpu(const Tensor& x) {
    Tensor y(x.shape(), Device::cpu());
    const float* px = x.data();
    float* py = y.data();
    for (int64_t i = 0; i < x.numel(); ++i) py[i] = px[i] > 0.f ? px[i] : 0.f;
    return y;
}

static Tensor relu_backward_cpu(const Tensor& x, const Tensor& gy) {
    if (x.numel() != gy.numel()) throw std::runtime_error("size mismatch");
    Tensor gx(x.shape(), Device::cpu());
    const float* px = x.data();
    const float* pgy = gy.data();
    float* pgx = gx.data();
    for (int64_t i = 0; i < x.numel(); ++i) pgx[i] = (px[i] > 0.f) ? pgy[i] : 0.f;
    return gx;
}

static Tensor sigmoid_forward_cpu(const Tensor& x) {
    Tensor y(x.shape(), Device::cpu());
    const float* px = x.data();
    float* py = y.data();
    for (int64_t i = 0; i < x.numel(); ++i) {
        float v = px[i];
        py[i] = 1.0f / (1.0f + std::exp(-v));
    }
    return y;
}

static Tensor sigmoid_backward_cpu(const Tensor& y, const Tensor& gy) {
    if (y.numel() != gy.numel()) throw std::runtime_error("size mismatch");
    Tensor gx(y.shape(), Device::cpu());
    const float* py = y.data();
    const float* pgy = gy.data();
    float* pgx = gx.data();
    for (int64_t i = 0; i < y.numel(); ++i) {
        pgx[i] = pgy[i] * py[i] * (1.0f - py[i]); 
    }
    return gx;
}

//  CUDA 实现 
#ifdef __CUDACC__
__global__ void relu_fwd_kernel(const float* x, float* y, int64_t n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) y[i] = x[i] > 0.f ? x[i] : 0.f;
}

__global__ void relu_bwd_kernel(const float* x, const float* gy, float* gx, int64_t n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) gx[i] = (x[i] > 0.f) ? gy[i] : 0.f;
}

__global__ void sigmoid_fwd_kernel(const float* x, float* y, int64_t n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) {
        float v = x[i];
        y[i] = 1.0f / (1.0f + __expf(-v));
    }
}

__global__ void sigmoid_bwd_kernel(const float* y, const float* gy, float* gx, int64_t n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) {
        gx[i] = gy[i] * y[i] * (1.0f - y[i]);
    }
}

static int grid_for(int64_t n) {
    const int block = 256;
    return static_cast<int>((n + block - 1) / block);
}

static Tensor relu_forward_gpu(const Tensor& x) {
    Tensor y(x.shape(), x.device());
    int64_t n = x.numel();
    if (n == 0) return y;
    CUDA_CHECK(cudaSetDevice(x.device().index));
    relu_fwd_kernel<<<grid_for(n), 256>>>(x.data(), y.data(), n);
    CUDA_CHECK(cudaGetLastError());
    return y;
}

static Tensor relu_backward_gpu(const Tensor& x, const Tensor& gy) {
    Tensor gx(x.shape(), x.device());
    int64_t n = x.numel();
    CUDA_CHECK(cudaSetDevice(x.device().index));
    relu_bwd_kernel<<<grid_for(n), 256>>>(x.data(), gy.data(), gx.data(), n);
    CUDA_CHECK(cudaGetLastError());
    return gx;
}

static Tensor sigmoid_forward_gpu(const Tensor& x) {
    Tensor y(x.shape(), x.device());
    int64_t n = x.numel();
    CUDA_CHECK(cudaSetDevice(x.device().index));
    sigmoid_fwd_kernel<<<grid_for(n), 256>>>(x.data(), y.data(), n);
    CUDA_CHECK(cudaGetLastError());
    return y;
}

static Tensor sigmoid_backward_gpu(const Tensor& y, const Tensor& gy) {
    Tensor gx(y.shape(), y.device());
    int64_t n = y.numel();
    CUDA_CHECK(cudaSetDevice(y.device().index));
    sigmoid_bwd_kernel<<<grid_for(n), 256>>>(y.data(), gy.data(), gx.data(), n);
    CUDA_CHECK(cudaGetLastError());
    return gx;
}
#endif

Tensor relu_forward(const Tensor& x) {
    if (x.device().type == DeviceType::CPU) return relu_forward_cpu(x);
#ifdef __CUDACC__
    return relu_forward_gpu(x);
#else
    throw std::runtime_error("CUDA not available");
#endif
}

Tensor relu_backward(const Tensor& x, const Tensor& grad_y) {
    if (x.device().type != grad_y.device().type) throw std::runtime_error("device mismatch");
    if (x.numel() != grad_y.numel()) throw std::runtime_error("size mismatch");
    if (x.device().type == DeviceType::CPU) return relu_backward_cpu(x, grad_y);
#ifdef __CUDACC__
    return relu_backward_gpu(x, grad_y);
#else
    throw std::runtime_error("CUDA not available");
#endif
}

Tensor sigmoid_forward(const Tensor& x) {
    if (x.device().type == DeviceType::CPU) return sigmoid_forward_cpu(x);
#ifdef __CUDACC__
    return sigmoid_forward_gpu(x);
#else
    throw std::runtime_error("CUDA not available");
#endif
}

Tensor sigmoid_backward(const Tensor& y, const Tensor& grad_y) {
    if (y.device().type != grad_y.device().type) throw std::runtime_error("device mismatch");
    if (y.numel() != grad_y.numel()) throw std::runtime_error("size mismatch");
    if (y.device().type == DeviceType::CPU) return sigmoid_backward_cpu(y, grad_y);
#ifdef __CUDACC__
    return sigmoid_backward_gpu(y, grad_y);
#else
    throw std::runtime_error("CUDA not available");
#endif
}
==================== END OF FILE: cuda_src/activations.cu ====================

==================== START OF FILE: cuda_src/elementwise.h ====================
#pragma once
#include "tensor.h"

void elementwise_add(const Tensor& a, const Tensor& b, Tensor& c);
void elementwise_add_backward(const Tensor& grad_out, Tensor& grad_a, Tensor& grad_b);
==================== END OF FILE: cuda_src/elementwise.h ====================

==================== START OF FILE: cuda_src/elementwise.cu ====================
#include "elementwise.h"
#include <cuda_runtime.h>
#include <stdexcept>

#define CUDA_CHECK(cmd) do { \
    cudaError_t e = (cmd); \
    if (e != cudaSuccess) throw std::runtime_error(cudaGetErrorString(e)); \
} while(0)

__global__ void add_kernel(const float* a, const float* b, float* c, int64_t n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) {
        c[i] = a[i] + b[i];
    }
}

__global__ void add_backward_kernel(const float* grad_out, float* grad_a, float* grad_b, int64_t n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) {
        grad_a[i] = grad_out[i];
        grad_b[i] = grad_out[i];
    }
}

void elementwise_add(const Tensor& a, const Tensor& b, Tensor& c) {
    if (a.numel() != b.numel() || a.numel() != c.numel()) {
        throw std::runtime_error("Tensor size mismatch in elementwise_add");
    }
    
    if (a.device().type != DeviceType::GPU || 
        b.device().type != DeviceType::GPU || 
        c.device().type != DeviceType::GPU) {
        throw std::runtime_error("elementwise_add requires GPU tensors");
    }
    
    int64_t n = a.numel();
    int threads = 256;
    int blocks = (n + threads - 1) / threads;
    
    CUDA_CHECK(cudaSetDevice(a.device().index));
    add_kernel<<<blocks, threads>>>(a.data(), b.data(), c.data(), n);
    CUDA_CHECK(cudaGetLastError());
}

void elementwise_add_backward(const Tensor& grad_out, Tensor& grad_a, Tensor& grad_b) {
    if (grad_out.device().type != DeviceType::GPU) {
        throw std::runtime_error("elementwise_add_backward requires GPU tensor");
    }
    
    int64_t n = grad_out.numel();
    int threads = 256;
    int blocks = (n + threads - 1) / threads;
    
    CUDA_CHECK(cudaSetDevice(grad_out.device().index));
    add_backward_kernel<<<blocks, threads>>>(grad_out.data(), grad_a.data(), grad_b.data(), n);
    CUDA_CHECK(cudaGetLastError());
}
==================== END OF FILE: cuda_src/elementwise.cu ====================

==================== START OF FILE: cuda_src/tensor.cu ====================
#include "tensor.h"
#include <numeric>
#include <cstdlib>
#include <cstring>
#include <stdexcept>
#include <cuda_runtime.h>

#define CUDA_CHECK(cmd) do { \
    cudaError_t e = (cmd); \
    if (e != cudaSuccess) throw std::runtime_error(cudaGetErrorString(e)); \
} while(0)

static int64_t product(const std::vector<int64_t>& v) {
    if (v.empty()) return 0;
    int64_t p = 1;
    for (auto x : v) p *= x;
    return p;
}

Tensor::Tensor(const std::vector<int64_t>& shape, Device device)
: shape_(shape), numel_(product(shape)), device_(device) {
    allocate();
}

Tensor::~Tensor() { deallocate(); }

Tensor::Tensor(Tensor&& other) noexcept {
    shape_   = std::move(other.shape_);
    numel_   = other.numel_;
    device_  = other.device_;
    storage_ = other.storage_;
    other.numel_ = 0;
    other.storage_ = nullptr;
}

Tensor& Tensor::operator=(Tensor&& other) noexcept {
    if (this != &other) {
        deallocate();
        shape_   = std::move(other.shape_);
        numel_   = other.numel_;
        device_  = other.device_;
        storage_ = other.storage_;
        other.numel_ = 0;
        other.storage_ = nullptr;
    }
    return *this;
}

void Tensor::allocate() {
    if (numel_ <= 0) return;
    size_t nbytes = static_cast<size_t>(numel_) * sizeof(float);
    if (device_.type == DeviceType::CPU) {
        storage_ = std::malloc(nbytes);
        if (!storage_) throw std::bad_alloc();
    } else {
        CUDA_CHECK(cudaSetDevice(device_.index));
        CUDA_CHECK(cudaMalloc(&storage_, nbytes));
    }
}

void Tensor::deallocate() {
    if (!storage_) return;
    if (device_.type == DeviceType::CPU) {
        std::free(storage_);
    } else {
        CUDA_CHECK(cudaSetDevice(device_.index));
        CUDA_CHECK(cudaFree(storage_));
    }
    storage_ = nullptr;
}

float* Tensor::data() { return reinterpret_cast<float*>(storage_); }
const float* Tensor::data() const { return reinterpret_cast<const float*>(storage_); }

void Tensor::copy_to(Tensor& dst) const {
    if (numel_ != dst.numel_) throw std::runtime_error("size mismatch in copy_to");
    size_t nbytes = static_cast<size_t>(numel_) * sizeof(float);

    if (device_.type == DeviceType::CPU && dst.device_.type == DeviceType::CPU) {
        std::memcpy(dst.storage_, storage_, nbytes);
    } else if (device_.type == DeviceType::CPU && dst.device_.type == DeviceType::GPU) {
        CUDA_CHECK(cudaSetDevice(dst.device_.index));
        CUDA_CHECK(cudaMemcpy(dst.storage_, storage_, nbytes, cudaMemcpyHostToDevice));
    } else if (device_.type == DeviceType::GPU && dst.device_.type == DeviceType::CPU) {
        CUDA_CHECK(cudaSetDevice(device_.index));
        CUDA_CHECK(cudaMemcpy(dst.storage_, storage_, nbytes, cudaMemcpyDeviceToHost));
    } else if (device_.type == DeviceType::GPU && dst.device_.type == DeviceType::GPU) {
        CUDA_CHECK(cudaSetDevice(device_.index));
        CUDA_CHECK(cudaMemcpy(dst.storage_, storage_, nbytes, cudaMemcpyDeviceToDevice));
    }
}

Tensor Tensor::cpu() const {
    Tensor out(shape_, Device::cpu());
    if (numel_ > 0) copy_to(out);
    return out;
}

Tensor Tensor::gpu(int idx) const {
    Tensor out(shape_, Device::gpu(idx));
    if (numel_ > 0) copy_to(out);
    return out;
}

void Tensor::fill(float v) {
    if (numel_ <= 0) return;
    if (device_.type == DeviceType::CPU) {
        float* p = data();
        for (int64_t i = 0; i < numel_; ++i) p[i] = v;
    } else {
        std::vector<float> tmp(static_cast<size_t>(numel_), v);
        Tensor host(shape_, Device::cpu());
        std::memcpy(host.data(), tmp.data(), tmp.size() * sizeof(float));
        host.copy_to(*this);
    }
}
==================== END OF FILE: cuda_src/tensor.cu ====================

==================== START OF FILE: cuda_src/layers.h ====================
#pragma once
#include "tensor.h"

// 1. Activation: Sigmoid
void sigmoid_forward(const Tensor& input, Tensor& output);
// Sigmoid Backward: grad_input = grad_output * y * (1-y)
void sigmoid_backward(const Tensor& output, const Tensor& grad_output, Tensor& grad_input);

// 2. Activation: ReLU
void relu_forward(const Tensor& input, Tensor& output);
void relu_backward(const Tensor& input, const Tensor& grad_output, Tensor& grad_input);

// 3. Linear (Fully Connected)
// Y = X * W^T + b
void linear_forward(const Tensor& input, const Tensor& weight, const Tensor& bias, Tensor& output);
// Linear Backward involves computing dX, dW, db
void linear_backward(const Tensor& input, const Tensor& weight, const Tensor& grad_output, 
                     Tensor& grad_input, Tensor& grad_weight, Tensor& grad_bias);

// 4. Convolution
void conv2d_forward(const Tensor& input, const Tensor& weight, Tensor& output, int stride, int padding);

// 5. Max Pooling
void maxpool_forward(const Tensor& input, Tensor& output, Tensor& mask, 
                     int k, int s, int p);
void maxpool_backward(const Tensor& grad_output, const Tensor& mask, Tensor& grad_input, 
                      int k, int s, int p);

// 6. Softmax
void softmax_forward(const Tensor& input, Tensor& output);

// 7. Cross Entropy Loss
float cross_entropy_loss(const Tensor& probs, const Tensor& labels);
void cross_entropy_backward(const Tensor& probs, const Tensor& labels, Tensor& grad_input);
==================== END OF FILE: cuda_src/layers.h ====================

==================== START OF FILE: cuda_src/layers.cu ====================
#include "layers.h"
#include <cuda_runtime.h>
#include <cublas_v2.h>
#include <cmath>
#include <cfloat>
#include <vector>

// 辅助：获取 CUDA 线程网格
inline void get_grid(int n, int& blocks, int& threads) {
    threads = 256;
    blocks = (n + threads - 1) / threads;
}

// ==========================================
// 1. Sigmoid
// ==========================================
__global__ void sigmoid_fwd_kernel(int n, const float* x, float* y) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) y[i] = 1.0f / (1.0f + expf(-x[i]));
}

void sigmoid_forward(const Tensor& input, Tensor& output) {
    int n = input.numel();
    int b, t; get_grid(n, b, t);
    sigmoid_fwd_kernel<<<b, t>>>(n, input.data(), output.data());
}

// grad_in = grad_out * y * (1 - y)
__global__ void sigmoid_bwd_kernel(int n, const float* y, const float* grad_y, float* grad_x) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) {
        float y_val = y[i];
        grad_x[i] = grad_y[i] * y_val * (1.0f - y_val);
    }
}

void sigmoid_backward(const Tensor& output, const Tensor& grad_output, Tensor& grad_input) {
    int n = output.numel();
    int b, t; get_grid(n, b, t);
    sigmoid_bwd_kernel<<<b, t>>>(n, output.data(), grad_output.data(), grad_input.data());
}

// ==========================================
// 2. ReLU
// ==========================================
__global__ void relu_fwd_kernel(int n, const float* x, float* y) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) y[i] = x[i] > 0.0f ? x[i] : 0.0f;
}

void relu_forward(const Tensor& input, Tensor& output) {
    int n = input.numel();
    int b, t; get_grid(n, b, t);
    relu_fwd_kernel<<<b, t>>>(n, input.data(), output.data());
}

__global__ void relu_bwd_kernel(int n, const float* x, const float* grad_y, float* grad_x) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) grad_x[i] = x[i] > 0.0f ? grad_y[i] : 0.0f;
}

void relu_backward(const Tensor& input, const Tensor& grad_output, Tensor& grad_input) {
    int n = input.numel();
    int b, t; get_grid(n, b, t);
    relu_bwd_kernel<<<b, t>>>(n, input.data(), grad_output.data(), grad_input.data());
}

// ==========================================
// 3. Linear (Fully Connected) using cuBLAS
// Y = X * W^T + b
// ==========================================
// bias kernel: add bias to each row
__global__ void add_bias_kernel(const float* bias, float* output, int N, int OutFeatures) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < N * OutFeatures) {
        int col = idx % OutFeatures;
        output[idx] += bias[col];
    }
}

void linear_forward(const Tensor& input, const Tensor& weight, const Tensor& bias, Tensor& output) {
    // Input: [N, InF], Weight: [OutF, InF], Output: [N, OutF]
    int N = input.shape()[0];
    int InF = input.shape()[1];
    int OutF = weight.shape()[0];

    cublasHandle_t handle; cublasCreate(&handle);
    float alpha = 1.0f, beta = 0.0f;

    // 1. GEMM: C = A * B^T
    // cuBLAS is col-major. C = B^T * A^T (in memory) -> logically C = A * B^T
    // We want Output(N, OutF) = Input(N, InF) * Weight^T(InF, OutF)
    // In col-major logic: Output^T = Weight * Input^T
    cublasSgemm(handle, CUBLAS_OP_T, CUBLAS_OP_N,
                OutF, N, InF,
                &alpha,
                weight.data(), InF,  // A (transposed -> normal in col-major)
                input.data(), InF,   // B
                &beta,
                output.data(), OutF); // C

    // 2. Add Bias
    int b, t; get_grid(N * OutF, b, t);
    add_bias_kernel<<<b, t>>>(bias.data(), output.data(), N, OutF);
    
    cublasDestroy(handle);
}

// Linear Backward (Simplified for assignment)
// dInput = dOutput * W
// dWeight = dOutput^T * Input
// dBias = sum(dOutput, axis=0)
__global__ void reduce_sum_bias_kernel(const float* grad_out, float* grad_bias, int N, int OutF) {
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    if (col < OutF) {
        float sum = 0.0f;
        for (int i = 0; i < N; ++i) sum += grad_out[i * OutF + col];
        grad_bias[col] = sum;
    }
}

void linear_backward(const Tensor& input, const Tensor& weight, const Tensor& grad_output, 
                     Tensor& grad_input, Tensor& grad_weight, Tensor& grad_bias) {
    int N = input.shape()[0];
    int InF = input.shape()[1];
    int OutF = weight.shape()[0];
    
    cublasHandle_t handle; cublasCreate(&handle);
    float alpha = 1.0f, beta = 0.0f;

    // 1. dInput = dOutput * W
    cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N,
                InF, N, OutF,
                &alpha,
                weight.data(), InF,
                grad_output.data(), OutF,
                &beta,
                grad_input.data(), InF);

    // 2. dWeight = dOutput^T * Input
    cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_T,
                InF, OutF, N,
                &alpha,
                input.data(), InF,
                grad_output.data(), OutF,
                &beta,
                grad_weight.data(), InF);

    // 3. dBias
    int b, t; get_grid(OutF, b, t);
    reduce_sum_bias_kernel<<<b, t>>>(grad_output.data(), grad_bias.data(), N, OutF);

    cublasDestroy(handle);
}

// ==========================================
// 4. Convolution
// ==========================================

__global__ void im2col_kernel(const int n, const float* data_im,
    const int height, const int width, const int channels,
    const int kernel_h, const int kernel_w,
    const int pad_h, const int pad_w,
    const int stride_h, const int stride_w,
    const int height_col, const int width_col,
    float* data_col) {
    
    int index = blockIdx.x * blockDim.x + threadIdx.x;
    if (index >= n) return;

    int w_out = index % width_col;
    int h_index = index / width_col;
    int h_out = h_index % height_col;
    int channel_in = h_index / height_col;
    int channel_out = channel_in * kernel_h * kernel_w;
    int h_in = h_out * stride_h - pad_h;
    int w_in = w_out * stride_w - pad_w;

    float* data_col_ptr = data_col;
    data_col_ptr += (channel_out * height_col * width_col + h_out * width_col + w_out);
    const float* data_im_ptr = data_im + (channel_in * height * width);

    for (int i = 0; i < kernel_h; ++i) {
        for (int j = 0; j < kernel_w; ++j) {
            int h = h_in + i;
            int w = w_in + j;
            *data_col_ptr = (h >= 0 && w >= 0 && h < height && w < width) ?
                data_im_ptr[h * width + w] : 0;
            data_col_ptr += height_col * width_col;
        }
    }
}

__global__ void col2im_kernel(const int n, const float* data_col,
                              const int height, const int width, const int channels,
                              const int kernel_h, const int kernel_w,
                              const int pad_h, const int pad_w,
                              const int stride_h, const int stride_w,
                              const int height_col, const int width_col,
                              float* data_im) {
    int index = blockIdx.x * blockDim.x + threadIdx.x;
    if (index >= n) return;

    float val = 0;
    int w = index % width + pad_w;
    int h = (index / width) % height + pad_h;
    int c = index / (width * height);

    int w_col_start = (w < kernel_w) ? 0 : (w - kernel_w) / stride_w + 1;
    int w_col_end = min(w / stride_w + 1, width_col);
    int h_col_start = (h < kernel_h) ? 0 : (h - kernel_h) / stride_h + 1;
    int h_col_end = min(h / stride_h + 1, height_col);

    for (int h_col = h_col_start; h_col < h_col_end; ++h_col) {
        for (int w_col = w_col_start; w_col < w_col_end; ++w_col) {
            int h_k = h - h_col * stride_h;
            int w_k = w - w_col * stride_w;
            if (h_k % 1 == 0 && w_k % 1 == 0) { // simple check
                int data_col_index = (((c * kernel_h + h_k) * kernel_w + w_k) * height_col + h_col) * width_col + w_col;
                val += data_col[data_col_index];
            }
        }
    }
    data_im[index] = val;
}

void conv2d_forward(const Tensor& input, const Tensor& weight, Tensor& output, int stride, int padding) {
    int batch = input.shape()[0];
    int in_c = input.shape()[1];
    int in_h = input.shape()[2];
    int in_w = input.shape()[3];
    int out_c = weight.shape()[0];
    int k_h = weight.shape()[2];
    int k_w = weight.shape()[3];
    int out_h = output.shape()[2];
    int out_w = output.shape()[3];

    cublasHandle_t handle; cublasCreate(&handle);
    float* d_col;
    cudaMalloc(&d_col, in_c * k_h * k_w * out_h * out_w * sizeof(float));

    for (int n = 0; n < batch; ++n) {
        int num_kernels = in_c * out_h * out_w;
        int b, t; get_grid(num_kernels, b, t);
        im2col_kernel<<<b, t>>>(num_kernels, input.data() + n*in_c*in_h*in_w, 
                                in_h, in_w, in_c, k_h, k_w, padding, padding, stride, stride,
                                out_h, out_w, d_col);
        
        float alpha = 1.0f, beta = 0.0f;
        // Output = Weight * Col
        // [OutC, OutHW] = [OutC, K] * [K, OutHW]
        int m = out_h * out_w;
        int k = in_c * k_h * k_w;
        int n_dim = out_c;
        
        cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N,
                    m, n_dim, k,
                    &alpha, 
                    d_col, m,
                    weight.data(), k, 
                    &beta, 
                    output.data() + n*out_c*out_h*out_w, m);
    }
    cudaFree(d_col);
    cublasDestroy(handle);
}

void conv2d_backward(const Tensor& input, const Tensor& weight, const Tensor& grad_output, 
                     Tensor& grad_input, Tensor& grad_weight, int stride, int padding) {
    int batch = input.shape()[0];
    int in_c = input.shape()[1];
    int in_h = input.shape()[2];
    int in_w = input.shape()[3];
    int out_c = weight.shape()[0];
    int k_h = weight.shape()[2];
    int k_w = weight.shape()[3];
    int out_h = grad_output.shape()[2];
    int out_w = grad_output.shape()[3];

    cublasHandle_t handle; cublasCreate(&handle);
    float* d_col;
    int col_size = in_c * k_h * k_w * out_h * out_w;
    cudaMalloc(&d_col, col_size * sizeof(float));

    int m = out_h * out_w;
    int k = in_c * k_h * k_w;
    int n_dim = out_c; // Output Channels

    float alpha = 1.0f, beta = 0.0f, beta_one = 1.0f;

    grad_weight.fill(0.0f); // Accumulate over batch

    for (int n = 0; n < batch; ++n) {
        // 1. Re-compute im2col for inputs
        int num_kernels = in_c * out_h * out_w;
        int b, t; get_grid(num_kernels, b, t);
        im2col_kernel<<<b, t>>>(num_kernels, input.data() + n*in_c*in_h*in_w, 
                                in_h, in_w, in_c, k_h, k_w, padding, padding, stride, stride,
                                out_h, out_w, d_col);

        // 2. dWeight = dOutput * Col^T
        // We want result [K, OutC] (stored as OutC rows of K if row-major, or K rows of OutC col-major) 
        // Logic: dW(K, OutC) = Col(M, K)^T * dOut(M, OutC)
        cublasSgemm(handle, CUBLAS_OP_T, CUBLAS_OP_N,
                    k, n_dim, m,
                    &alpha,
                    d_col, m,
                    grad_output.data() + n*out_c*m, m,
                    &beta_one, // Aggregate gradients
                    grad_weight.data(), k);

        // 3. dInput: Compute dCol = W^T * dOutput
        // dCol(M, K) = dOut(M, OutC) * W(K, OutC)^T (Transposed weight)
        cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_T,
                    m, k, n_dim,
                    &alpha,
                    grad_output.data() + n*out_c*m, m,
                    weight.data(), k,
                    &beta,
                    d_col, m);
        
        // Col2Im
        int num_im = in_c * in_h * in_w; 
        get_grid(num_im, b, t);
        col2im_kernel<<<b, t>>>(num_im, d_col, in_h, in_w, in_c, k_h, k_w, 
                                padding, padding, stride, stride, out_h, out_w, 
                                grad_input.data() + n*in_c*in_h*in_w);
    }
    
    cudaFree(d_col);
    cublasDestroy(handle);
}

// ==========================================
// 5. Max Pooling
// ==========================================
__global__ void maxpool_fwd_kernel(int nthreads, const float* bottom, int c, int h, int w, 
                                   int ph, int pw, int kh, int kw, int sh, int sw, int pad,
                                   float* top, float* mask) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= nthreads) return;
    int pw_idx = idx % pw;
    int ph_idx = (idx / pw) % ph;
    int c_idx = (idx / pw / ph) % c;
    int n_idx = idx / pw / ph / c;

    int hstart = ph_idx * sh - pad;
    int wstart = pw_idx * sw - pad;
    int hend = min(hstart + kh, h);
    int wend = min(wstart + kw, w);
    hstart = max(hstart, 0); wstart = max(wstart, 0);

    float maxval = -FLT_MAX;
    int maxidx = -1;
    const float* src = bottom + (n_idx*c + c_idx)*h*w;
    for (int i = hstart; i < hend; ++i) {
        for (int j = wstart; j < wend; ++j) {
            if (src[i*w + j] > maxval) {
                maxval = src[i*w + j];
                maxidx = i*w + j;
            }
        }
    }
    top[idx] = maxval;
    mask[idx] = (float)(maxidx + (n_idx*c + c_idx)*h*w);
}

void maxpool_forward(const Tensor& input, Tensor& output, Tensor& mask, int k, int s, int p) {
    int n = output.numel();
    int b, t; get_grid(n, b, t);
    maxpool_fwd_kernel<<<b, t>>>(n, input.data(), input.shape()[1], input.shape()[2], input.shape()[3],
                                 output.shape()[2], output.shape()[3], k, k, s, s, p, 
                                 output.data(), mask.data());
}

__global__ void maxpool_bwd_kernel(int n, const float* top_diff, const float* mask, float* bottom_diff) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) atomicAdd(bottom_diff + (int)mask[i], top_diff[i]);
}

void maxpool_backward(const Tensor& grad_output, const Tensor& mask, Tensor& grad_input, int k, int s, int p) {
    grad_input.fill(0.0f);
    int n = grad_output.numel();
    int b, t; get_grid(n, b, t);
    maxpool_bwd_kernel<<<b, t>>>(n, grad_output.data(), mask.data(), grad_input.data());
}

// ==========================================
// 6. Softmax & 7. CrossEntropy
// ==========================================
__global__ void softmax_kernel(const float* x, float* y, int N, int C) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < N) {
        const float* row_x = x + i*C;
        float* row_y = y + i*C;
        float maxv = -FLT_MAX;
        for(int j=0; j<C; ++j) maxv = max(maxv, row_x[j]);
        float sum = 0.0f;
        for(int j=0; j<C; ++j) {
            row_y[j] = expf(row_x[j] - maxv);
            sum += row_y[j];
        }
        for(int j=0; j<C; ++j) row_y[j] /= sum;
    }
}

void softmax_forward(const Tensor& input, Tensor& output) {
    int N = input.shape()[0];
    int C = input.shape()[1];
    int b, t; get_grid(N, b, t);
    softmax_kernel<<<b, t>>>(input.data(), output.data(), N, C);
}

__global__ void ce_kernel(const float* p, const float* label, float* loss, int N, int C) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < N) {
        int l = (int)label[i];
        atomicAdd(loss, -logf(p[i*C + l] + 1e-7f));
    }
}

float cross_entropy_loss(const Tensor& probs, const Tensor& labels) {
    int N = probs.shape()[0];
    int C = probs.shape()[1];
    float h_loss = 0.0f;
    float* d_loss; cudaMalloc(&d_loss, sizeof(float));
    cudaMemcpy(d_loss, &h_loss, sizeof(float), cudaMemcpyHostToDevice);
    
    int b, t; get_grid(N, b, t);
    ce_kernel<<<b, t>>>(probs.data(), labels.data(), d_loss, N, C);
    
    cudaMemcpy(&h_loss, d_loss, sizeof(float), cudaMemcpyDeviceToHost);
    cudaFree(d_loss);
    return h_loss / N;
}

__global__ void ce_bwd_kernel(const float* p, const float* label, float* grad, int N, int C) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < N*C) {
        int n = idx / C;
        int c = idx % C;
        int l = (int)label[n];
        grad[idx] = (p[idx] - (c == l ? 1.0f : 0.0f)) / N;
    }
}

void cross_entropy_backward(const Tensor& probs, const Tensor& labels, Tensor& grad_input) {
    int N = probs.shape()[0];
    int C = probs.shape()[1];
    int b, t; get_grid(N*C, b, t);
    ce_bwd_kernel<<<b, t>>>(probs.data(), labels.data(), grad_input.data(), N, C);
}
==================== END OF FILE: cuda_src/layers.cu ====================

==================== START OF FILE: cuda_src/conv_layer.cu ====================
#include "conv_layer.h"
#include <iostream>


// Im2Col Kernel 
// 将输入[C, H, W]展开成矩阵列[C*K*K, H_out*W_out]

__global__ void im2col_kernel(const float* data_im, float* data_col,
                              int channels, int height, int width,
                              int kernel_h, int kernel_w,
                              int pad_h, int pad_w,
                              int stride_h, int stride_w,
                              int height_col, int width_col) {
    int index = blockIdx.x * blockDim.x + threadIdx.x;
    int n = height_col * width_col * channels * kernel_h * kernel_w;
    
    if (index < n) {
        int w_out = index % width_col;
        int h_out = (index / width_col) % height_col;
        int channel_in = index / (width_col * height_col);
        
        int channel = channel_in / (kernel_h * kernel_w);
        int k_h = (channel_in / kernel_w) % kernel_h;
        int k_w = channel_in % kernel_w;

        int h_in = h_out * stride_h - pad_h + k_h;
        int w_in = w_out * stride_w - pad_w + k_w;

        data_col[index] = (h_in >= 0 && w_in >= 0 && h_in < height && w_in < width) ?
                          data_im[h_in * width + w_in + channel * height * width] : 0;
    }
}

void im2col_gpu(const float* data_im, float* data_col,
                int channels, int height, int width,
                int ksize, int pad, int stride, 
                int height_col, int width_col) {
    int num_kernels = channels * ksize * ksize * height_col * width_col;
    int threads = 256;
    int blocks = (num_kernels + threads - 1) / threads;
    im2col_kernel<<<blocks, threads>>>(data_im, data_col, channels, height, width, 
                                       ksize, ksize, pad, pad, stride, stride, 
                                       height_col, width_col);
}


__global__ void col2im_kernel(const float* data_col, float* data_im,
                              int channels, int height, int width,
                              int kernel_h, int kernel_w,
                              int pad_h, int pad_w,
                              int stride_h, int stride_w,
                              int height_col, int width_col) {
    int index = blockIdx.x * blockDim.x + threadIdx.x;
    int n = height_col * width_col * channels * kernel_h * kernel_w;
    
    if (index < n) {
        float val = data_col[index];
        int w_out = index % width_col;
        int h_out = (index / width_col) % height_col;
        int channel_in = index / (width_col * height_col);
        
        int channel = channel_in / (kernel_h * kernel_w);
        int k_h = (channel_in / kernel_w) % kernel_h;
        int k_w = channel_in % kernel_w;

        int h_in = h_out * stride_h - pad_h + k_h;
        int w_in = w_out * stride_w - pad_w + k_w;

        if (h_in >= 0 && w_in >= 0 && h_in < height && w_in < width) {
            atomicAdd(data_im + (channel * height * width + h_in * width + w_in), val);
        }
    }
}

void col2im_gpu(const float* data_col, float* data_im,
                int channels, int height, int width,
                int ksize, int pad, int stride,
                int height_col, int width_col) {
    cudaMemset(data_im, 0, sizeof(float) * channels * height * width);

    int num_kernels = channels * ksize * ksize * height_col * width_col;
    int threads = 256;
    int blocks = (num_kernels + threads - 1) / threads;
    col2im_kernel<<<blocks, threads>>>(data_col, data_im, channels, height, width,
                                       ksize, ksize, pad, pad, stride, stride,
                                       height_col, width_col);
}


// Forward Convolution

void forward_conv(float* d_input, float* d_output, float* d_weight, float* d_col_buffer,
                  int batch_size, int in_channels, int out_channels, 
                  int height, int width, int ksize, int stride, int padding,
                  cublasHandle_t handle) {


    int height_out = (height + 2 * padding - ksize) / stride + 1;
    int width_out = (width + 2 * padding - ksize) / stride + 1;

    
    int m = height_out * width_out;
    int n = out_channels;
    int k = in_channels * ksize * ksize;


    float alpha = 1.0f;
    float beta = 0.0f;

    for (int b = 0; b < batch_size; ++b) {
        im2col_gpu(d_input + b * in_channels * height * width, 
                   d_col_buffer, 
                   in_channels, height, width, ksize, padding, stride, height_out, width_out);

        
        cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N,
                    m, n, k,
                    &alpha,
                    d_col_buffer, m,
                    d_weight, k,     
                    &beta,
                    d_output + b * out_channels * height_out * width_out, m); 
    }
}


//  Backward Convolution
void backward_conv(float* d_input, float* d_grad_output, float* d_weight, 
                   float* d_col_buffer, float* d_grad_input, float* d_grad_weight,
                   int batch_size, int in_channels, int out_channels, 
                   int height, int width, int ksize, int stride, int padding,
                   cublasHandle_t handle) {

    int height_out = (height + 2 * padding - ksize) / stride + 1;
    int width_out = (width + 2 * padding - ksize) / stride + 1;
    
    int m = height_out * width_out;
    int n = out_channels;
    int k = in_channels * ksize * ksize;

    float alpha = 1.0f;
    float beta = 0.0f;
    float beta_one = 1.0f; 

    cudaMemset(d_grad_weight, 0, sizeof(float) * out_channels * k);
    
    for(int i=0; i<batch_size; i++) {
       
        
        im2col_gpu(d_input + i * in_channels * height * width, d_col_buffer,
                   in_channels, height, width, ksize, padding, stride, height_out, width_out);
        
     
        cublasSgemm(handle, CUBLAS_OP_T, CUBLAS_OP_N,
                    k, n, m,
                    &alpha,
                    d_col_buffer, m,
                    d_grad_output + i * n * m, m,
                    &beta_one, 
                    d_grad_weight, k);


        cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N,
                    m, k, n,
                    &alpha,
                    d_grad_output + i * n * m, m,
                    d_weight, k,
                    &beta,
                    d_col_buffer, m); 
        
        col2im_gpu(d_col_buffer, 
                   d_grad_input + i * in_channels * height * width,
                   in_channels, height, width, ksize, padding, stride, height_out, width_out);
    }
}
==================== END OF FILE: cuda_src/conv_layer.cu ====================

==================== START OF FILE: train/train_cifar10.py ====================
import sys, os, time
import numpy as np
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'python'))

# 引入 C++ 扩展库
import my_deep_lib 

from basic_operator import Value
from nn import Module, Parameter, Conv2D, Linear, ReLU, MaxPool2D, cross_entropy_loss
from optim import SGD
from data import DataLoader, CIFAR10Dataset

class Flatten:
    def __call__(self, x):
        d = x.realize_cached_data()
        
        # === 终极修复：获取 shape ===
        # 1. 尝试直接获取属性 (Numpy 或 Property-based Tensor)
        shape_attr = getattr(d, "shape", None)
        
        if isinstance(shape_attr, (list, tuple)):
            self.shape = shape_attr
        # 2. 如果属性是 method (Method-based Tensor)，则调用它
        elif callable(shape_attr):
            self.shape = shape_attr()
        else:
            raise RuntimeError(f"Unknown shape type for data: {type(d)}")
            
        # 获取 numel
        if hasattr(d, "numel"):
             numel = d.numel()
        else:
             numel = d.size
        
        current_batch = self.shape[0]
        flatten_dim = numel // current_batch
        
        # DEBUG:
        if not hasattr(self, "printed"):
            print(f"DEBUG: Flatten shape: {self.shape} -> [Batch={current_batch}, Dim={flatten_dim}]")
            self.printed = True
        
        # 执行 Reshape
        if isinstance(d, my_deep_lib.Tensor):
            if numel % current_batch != 0:
                 raise RuntimeError(f"Flatten error: numel {numel} not divisible by batch {current_batch}")
            # C++ Tensor reshape(list)
            new_d = d.reshape([current_batch, flatten_dim]) 
        else:
            # Numpy
            new_d = d.reshape(current_batch, -1).astype(np.float32)
            
        r = Value()
        r._init(None, [x], cached_data=new_d)
        r.op = self
        return r

    def gradient(self, out_grad, node):
        g = out_grad.realize_cached_data()
        
        if isinstance(g, my_deep_lib.Tensor):
            target_shape = list(self.shape)
            dg = g.reshape(target_shape)
        else:
            dg = g.reshape(self.shape).astype(np.float32)
            
        gv = Value()
        gv._init(None, [], cached_data=dg)
        return (gv,)

flatten = Flatten()

class CNN(Module):
    def __init__(self):
        super().__init__()
        # Conv1: 3 -> 32
        self.conv1 = Conv2D(3, 32, 3, padding=1)
        self.relu1 = ReLU()
        self.pool1 = MaxPool2D(2, 2)
        
        # Conv2: 32 -> 64
        self.conv2 = Conv2D(32, 64, 3, padding=1)
        self.relu2 = ReLU()
        self.pool2 = MaxPool2D(2, 2)
        
        # Flatten 后尺寸: 64通道 * 8 * 8
        self.fc1 = Linear(64*8*8, 256)
        self.relu3 = ReLU()
        self.fc2 = Linear(256, 10)
    
    def forward(self, x):
        x = self.pool1(self.relu1(self.conv1(x)))
        x = self.pool2(self.relu2(self.conv2(x)))
        x = flatten(x)
        x = self.relu3(self.fc1(x))
        return self.fc2(x)

def main():
    print("Loading data...")
    if not os.path.exists('./data'):
        os.makedirs('./data')
        
    train_ds = CIFAR10Dataset('./data', train=True)
    test_ds = CIFAR10Dataset('./data', train=False)
    
    # 稍微调小 batch_size
    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)
    test_loader = DataLoader(test_ds, batch_size=64, shuffle=False)
    
    model = CNN()
    
    # 计算参数量
    total_params = 0
    for p in model.parameters():
        d = p.cached_data
        if hasattr(d, "numel"):
            total_params += d.numel()
        else:
            total_params += d.size
    print(f"Parameters: {total_params:,}")
    
    opt = SGD(model.parameters(), lr=0.001, momentum=0.9)
    
    print("Training...")
    
    for epoch in range(10):
        t0 = time.time()
        total_loss, correct, total = 0.0, 0, 0
        
        for i, (data, labels) in enumerate(train_loader):
            # Numpy -> GPU Tensor
            gpu_data = my_deep_lib.Tensor.from_numpy(data, my_deep_lib.Device.gpu(0))
            gpu_labels = my_deep_lib.Tensor.from_numpy(labels.astype(np.float32), my_deep_lib.Device.gpu(0))

            x = Value(); x._init(None, [], cached_data=gpu_data)
            y = Value(); y._init(None, [], cached_data=gpu_labels, requires_grad=False)
            
            logits = model(x)
            loss = cross_entropy_loss(logits, y)
            
            opt.zero_grad()
            loss.backward()
            opt.step()
            
            loss_val = loss.realize_cached_data()
            if hasattr(loss_val, "to_numpy"):
                loss_scalar = float(loss_val.to_numpy())
            else:
                loss_scalar = float(loss_val)
            total_loss += loss_scalar
            
            logits_data = logits.realize_cached_data()
            if hasattr(logits_data, "to_numpy"):
                logits_np = logits_data.to_numpy()
                labels_np = labels 
            else:
                logits_np = logits_data
                labels_np = labels
                
            pred = logits_np.argmax(axis=1)
            correct += (pred == labels_np).sum()
            total += len(labels)
            
            if (i+1) % 10 == 0:
                print(f"  Batch {i+1}/{len(train_loader)}, Loss: {total_loss/(i+1):.4f}, Acc: {100*correct/total:.2f}%")
        
        test_correct, test_total = 0, 0
        for data, labels in test_loader:
            gpu_data = my_deep_lib.Tensor.from_numpy(data, my_deep_lib.Device.gpu(0))
            x = Value(); x._init(None, [], cached_data=gpu_data)
            
            logits = model(x)
            logits_data = logits.realize_cached_data()
            if hasattr(logits_data, "to_numpy"):
                logits_np = logits_data.to_numpy()
            else:
                logits_np = logits_data
                
            pred = logits_np.argmax(axis=1)
            test_correct += (pred == labels).sum()
            test_total += len(labels)
        
        print(f"Epoch {epoch+1}: Loss={total_loss/len(train_loader):.4f}, "
              f"Train Acc={100*correct/total:.2f}%, Test Acc={100*test_correct/test_total:.2f}%, "
              f"Time={time.time()-t0:.1f}s")

if __name__ == "__main__":
    main()
==================== END OF FILE: train/train_cifar10.py ====================
