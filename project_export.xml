<?xml version="1.0" encoding="UTF-8"?>
<project>

  <!-- é¡¹ç›®å…ƒæ•°æ® -->
  <metadata>
    <name>deep_learning</name>
    <path>/home/lukas/aicoding/deep_learning</path>
    <export_time>2026-01-17T17:48:22.618391</export_time>
    <total_files>21</total_files>
    <total_size_bytes>50921</total_size_bytes>
    <total_size_mb>0.05</total_size_mb>
    <file_types>
      <type extension=".py" count="15"/>
      <type extension=".h" count="5"/>
      <type extension=".cpp" count="1"/>
    </file_types>
  </metadata>

  <!-- ç›®å½•ç»“æ„ -->
  <directory_structure>
    <tree><![CDATA[
deep_learning/
â”œâ”€â”€ cuda_src/
â”‚   â”œâ”€â”€ activations.cu
â”‚   â”œâ”€â”€ activations.h
â”‚   â”œâ”€â”€ conv_layer.cu
â”‚   â”œâ”€â”€ conv_layer.h
â”‚   â”œâ”€â”€ elementwise.cu
â”‚   â”œâ”€â”€ elementwisew.h
â”‚   â”œâ”€â”€ layers.cu
â”‚   â”œâ”€â”€ layers.h
â”‚   â”œâ”€â”€ tensor.cu
â”‚   â””â”€â”€ tensor.h
â”œâ”€â”€ models/
â”‚   â””â”€â”€ simple_cnn.py
â”œâ”€â”€ pybind/
â”‚   â””â”€â”€ pybind_wrapper.cpp
â”œâ”€â”€ python/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â””â”€â”€ device.py
â”‚   â”œâ”€â”€ cuda_ops/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ activations.py
â”‚   â”‚   â”œâ”€â”€ conv.py
â”‚   â”‚   â”œâ”€â”€ linear.py
â”‚   â”‚   â””â”€â”€ pooling.py
â”‚   â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ nn/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ activations.py
â”‚   â”‚   â”œâ”€â”€ conv.py
â”‚   â”‚   â”œâ”€â”€ linear.py
â”‚   â”‚   â”œâ”€â”€ loss.py
â”‚   â”‚   â””â”€â”€ module.py
â”‚   â”œâ”€â”€ optim/
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ train/
â””â”€â”€ packer.py
    ]]></tree>
  </directory_structure>

  <!-- æºä»£ç æ–‡ä»¶ -->
  <files>

    <file index="1">
      <path>cuda_src/activations.h</path>
      <name>activations.h</name>
      <extension>.h</extension>
      <size_bytes>259</size_bytes>
      <modified>2026-01-17T17:15:49.590450</modified>
      <encoding>utf-8</encoding>
      <lines>10</lines>
      <content><![CDATA[
#pragma once
#include "tensor.h"

// ReLU
Tensor relu_forward(const Tensor& x);
Tensor relu_backward(const Tensor& x, const Tensor& grad_y); 

// Sigmoid
Tensor sigmoid_forward(const Tensor& x);
Tensor sigmoid_backward(const Tensor& y, const Tensor& grad_y); 
]]></content>
    </file>

    <file index="2">
      <path>cuda_src/conv_layer.h</path>
      <name>conv_layer.h</name>
      <extension>.h</extension>
      <size_bytes>831</size_bytes>
      <modified>2026-01-17T17:16:31.582447</modified>
      <encoding>utf-8</encoding>
      <lines>22</lines>
      <content><![CDATA[
#ifndef CONV_LAYER_H
#define CONV_LAYER_H

#include <cuda_runtime.h>
#include <cublas_v2.h>

// å‰å‘ä¼ æ’­
// input: [batch, in_c, h, w]
// output: [batch, out_c, h_out, w_out]
void forward_conv(float* d_input, float* d_output, float* d_weight, float* d_col_buffer,
                  int batch_size, int in_channels, int out_channels, 
                  int height, int width, int ksize, int stride, int padding,
                  cublasHandle_t handle);

// åå‘ä¼ æ’­
void backward_conv(float* d_input, float* d_grad_output, float* d_weight, 
                   float* d_col_buffer, float* d_grad_input, float* d_grad_weight,
                   int batch_size, int in_channels, int out_channels, 
                   int height, int width, int ksize, int stride, int padding,
                   cublasHandle_t handle);

#endif
]]></content>
    </file>

    <file index="3">
      <path>cuda_src/elementwisew.h</path>
      <name>elementwisew.h</name>
      <extension>.h</extension>
      <size_bytes>187</size_bytes>
      <modified>2026-01-17T17:19:31.958399</modified>
      <encoding>utf-8</encoding>
      <lines>5</lines>
      <content><![CDATA[
#pragma once
#include "tensor.h"

void elementwise_add(const Tensor& a, const Tensor& b, Tensor& c);
void elementwise_add_backward(const Tensor& grad_out, Tensor& grad_a, Tensor& grad_b);
]]></content>
    </file>

    <file index="4">
      <path>cuda_src/layers.h</path>
      <name>layers.h</name>
      <extension>.h</extension>
      <size_bytes>1464</size_bytes>
      <modified>2026-01-17T17:15:27.402457</modified>
      <encoding>utf-8</encoding>
      <lines>34</lines>
      <content><![CDATA[
#pragma once
#include "tensor.h"

// 1. Activation: Sigmoid
void sigmoid_forward(const Tensor& input, Tensor& output);
// Sigmoid Backward: grad_input = grad_output * y * (1-y)
void sigmoid_backward(const Tensor& output, const Tensor& grad_output, Tensor& grad_input);

// 2. Activation: ReLU
void relu_forward(const Tensor& input, Tensor& output);
void relu_backward(const Tensor& input, const Tensor& grad_output, Tensor& grad_input);

// 3. Linear (Fully Connected)
// Y = X * W^T + b
void linear_forward(const Tensor& input, const Tensor& weight, const Tensor& bias, Tensor& output);
// Linear Backward involves computing dX, dW, db
void linear_backward(const Tensor& input, const Tensor& weight, const Tensor& grad_output, 
                     Tensor& grad_input, Tensor& grad_weight, Tensor& grad_bias);

// 4. Convolution
void conv2d_forward(const Tensor& input, const Tensor& weight, Tensor& output, int stride, int padding);

// 5. Max Pooling
void maxpool_forward(const Tensor& input, Tensor& output, Tensor& mask, 
                     int k, int s, int p);
void maxpool_backward(const Tensor& grad_output, const Tensor& mask, Tensor& grad_input, 
                      int k, int s, int p);

// 6. Softmax
void softmax_forward(const Tensor& input, Tensor& output);

// 7. Cross Entropy Loss
float cross_entropy_loss(const Tensor& probs, const Tensor& labels);
void cross_entropy_backward(const Tensor& probs, const Tensor& labels, Tensor& grad_input);
]]></content>
    </file>

    <file index="5">
      <path>cuda_src/tensor.h</path>
      <name>tensor.h</name>
      <extension>.h</extension>
      <size_bytes>1204</size_bytes>
      <modified>2026-01-17T17:15:30.718455</modified>
      <encoding>utf-8</encoding>
      <lines>50</lines>
      <content><![CDATA[
#pragma once
#include <vector>
#include <memory>
#include <cstdint>
#include <stdexcept>

enum class DeviceType { CPU, GPU };

struct Device {
    DeviceType type;
    int index;

    Device() : type(DeviceType::CPU), index(0) {} 

    Device(DeviceType t, int i = 0) : type(t), index(i) {}
    static Device cpu() { return Device(DeviceType::CPU); }
    static Device gpu(int i = 0) { return Device(DeviceType::GPU, i); }
};

class Tensor {
public:
    Tensor(const std::vector<int64_t>& shape, Device device);
    ~Tensor();
    
    Tensor(Tensor&& other) noexcept;
    Tensor& operator=(Tensor&& other) noexcept;
    Tensor(const Tensor&) = delete;
    Tensor& operator=(const Tensor&) = delete;

    void fill(float v);
    Tensor cpu() const;
    Tensor gpu(int idx = 0) const;
    
    float* data();
    const float* data() const;
    
    const std::vector<int64_t>& shape() const { return shape_; }
    int64_t numel() const { return numel_; }
    const Device& device() const { return device_; }

private:
    void allocate();
    void deallocate();
    void copy_to(Tensor& dst) const;

    std::vector<int64_t> shape_;
    int64_t numel_;
    Device device_;
    void* storage_ = nullptr;
};
]]></content>
    </file>

    <file index="6">
      <path>models/simple_cnn.py</path>
      <name>simple_cnn.py</name>
      <extension>.py</extension>
      <size_bytes>3700</size_bytes>
      <modified>2026-01-17T17:41:00.518047</modified>
      <encoding>utf-8</encoding>
      <lines>112</lines>
      <content><![CDATA[
"""
ç®€å•çš„ CNN æ¨¡å‹ï¼ˆç”¨äº CIFAR-10ï¼‰

æ¶æ„ï¼š
Conv(3->32) -> ReLU -> MaxPool -> Conv(32->64) -> ReLU -> MaxPool -> Flatten -> Linear(64*8*8->512) -> ReLU -> Linear(512->10)
"""
import numpy as np
from nn.module import Module, Parameter
from nn.conv import Conv2D, MaxPool2D
from nn.linear import Linear
from nn.activations import ReLU
from basic_operator import Value

class SimpleCNN(Module):
    """
    ç®€å•çš„å·ç§¯ç¥ç»ç½‘ç»œ
    
    è¾“å…¥ï¼š[batch, 3, 32, 32] (CIFAR-10 å›¾åƒ)
    è¾“å‡ºï¼š[batch, 10] (10 ä¸ªç±»åˆ«çš„ logits)
    """
    def __init__(self, num_classes=10):
        super().__init__()
        
        # ç¬¬ä¸€å±‚å·ç§¯å—ï¼š3 -> 32 channels
        self.conv1 = Conv2D(in_channels=3, out_channels=32, 
                           kernel_size=3, stride=1, padding=1)
        self.relu1 = ReLU()
        self.pool1 = MaxPool2D(kernel_size=2, stride=2)  # 32x32 -> 16x16
        
        # ç¬¬äºŒå±‚å·ç§¯å—ï¼š32 -> 64 channels
        self.conv2 = Conv2D(in_channels=32, out_channels=64,
                           kernel_size=3, stride=1, padding=1)
        self.relu2 = ReLU()
        self.pool2 = MaxPool2D(kernel_size=2, stride=2)  # 16x16 -> 8x8
        
        # å…¨è¿æ¥å±‚
        self.fc1 = Linear(in_features=64 * 8 * 8, out_features=512)
        self.relu3 = ReLU()
        self.fc2 = Linear(in_features=512, out_features=num_classes)
    
    def forward(self, x: Value) -> Value:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: [batch, 3, 32, 32]
        
        Returns:
            logits: [batch, num_classes]
        """
        # ç¬¬ä¸€ä¸ªå·ç§¯å—
        x = self.conv1(x)       # [batch, 32, 32, 32]
        x = self.relu1(x)
        x = self.pool1(x)       # [batch, 32, 16, 16]
        
        # ç¬¬äºŒä¸ªå·ç§¯å—
        x = self.conv2(x)       # [batch, 64, 16, 16]
        x = self.relu2(x)
        x = self.pool2(x)       # [batch, 64, 8, 8]
        
        # Flatten
        x_data = x.realize_cached_data()
        batch_size = x_data.shape[0]
        flattened = x_data.reshape(batch_size, -1)  # [batch, 64*8*8]
        x = Value.make_const(flattened, requires_grad=x.requires_grad)
        x._init(x.op, x.inputs, cached_data=flattened, requires_grad=x.requires_grad)
        
        # å…¨è¿æ¥å±‚
        x = self.fc1(x)         # [batch, 512]
        x = self.relu3(x)
        x = self.fc2(x)         # [batch, 10]
        
        return x

# ç®€åŒ–ç‰ˆï¼ˆæ›´å°çš„æ¨¡å‹ï¼Œè®­ç»ƒæ›´å¿«ï¼‰
class TinyCNN(Module):
    """
    æ›´å°çš„ CNNï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰
    
    Conv(3->16) -> ReLU -> MaxPool -> Conv(16->32) -> ReLU -> MaxPool -> Linear(32*8*8->10)
    """
    def __init__(self, num_classes=10):
        super().__init__()
        
        self.conv1 = Conv2D(3, 16, kernel_size=3, stride=1, padding=1)
        self.relu1 = ReLU()
        self.pool1 = MaxPool2D(kernel_size=2, stride=2)
        
        self.conv2 = Conv2D(16, 32, kernel_size=3, stride=1, padding=1)
        self.relu2 = ReLU()
        self.pool2 = MaxPool2D(kernel_size=2, stride=2)
        
        self.fc = Linear(32 * 8 * 8, num_classes)
    
    def forward(self, x: Value) -> Value:
        x = self.conv1(x)
        x = self.relu1(x)
        x = self.pool1(x)
        
        x = self.conv2(x)
        x = self.relu2(x)
        x = self.pool2(x)
        
        # Flatten
        x_data = x.realize_cached_data()
        batch_size = x_data.shape[0]
        flattened = x_data.reshape(batch_size, -1)
        x = Value.make_const(flattened, requires_grad=x.requires_grad)
        x._init(x.op, x.inputs, cached_data=flattened, requires_grad=x.requires_grad)
        
        x = self.fc(x)
        return x
]]></content>
    </file>

    <file index="7">
      <path>packer.py</path>
      <name>packer.py</name>
      <extension>.py</extension>
      <size_bytes>11037</size_bytes>
      <modified>2026-01-17T17:47:59.889936</modified>
      <encoding>utf-8</encoding>
      <lines>333</lines>
      <content><![CDATA[
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é¡¹ç›®ä»£ç å¯¼å‡ºè„šæœ¬ - XMLæ ¼å¼ï¼ˆLLMä¼˜åŒ–ç‰ˆï¼‰
"""

import os
from pathlib import Path
from datetime import datetime
import xml.sax.saxutils as saxutils

# ==================== é…ç½®åŒºåŸŸ ====================
OUTPUT_FILENAME = "project_export.xml"

# åŒ…å«çš„æ–‡ä»¶æ‰©å±•å
INCLUDE_EXTENSIONS = {
    '.py', '.js', '.jsx', '.ts', '.tsx', '.java', '.c', '.cpp', '.h', '.hpp',
    '.cs', '.go', '.rs', '.php', '.rb', '.swift', '.kt', '.scala',
    '.html', '.css', '.scss', '.sass', '.less', '.vue', '.svelte',
    '.json', '.xml', '.yaml', '.yml', '.toml', '.ini', '.cfg', '.conf',
    '.md', '.txt', '.rst', '.sh', '.bash', '.sql', '.r', '.dart',
    '.gradle', '.properties', '.env', '.dockerfile'
}

# æ’é™¤çš„æ–‡ä»¶å¤¹
EXCLUDE_DIRS = {
    'node_modules', 'venv', 'env', '.venv', '.env',
    '.git', '.svn', '.hg', '__pycache__', '.pytest_cache',
    'dist', 'build', 'target', 'out', 'output',
    '.idea', '.vscode', '.vs', '.settings',
    'bin', 'obj', 'pkg', 'vendor',
    '.next', '.nuxt', 'coverage', '.nyc_output',
    '.gradle', '.cache', '.mypy_cache', '.tox'
}

# æ’é™¤çš„æ–‡ä»¶
EXCLUDE_FILES = {
    '.DS_Store', 'Thumbs.db', 'desktop.ini',
    '.gitignore', '.gitkeep', '.dockerignore',
    OUTPUT_FILENAME,
    'package-lock.json', 'yarn.lock', 'poetry.lock', 'Pipfile.lock'
}

# æ— æ‰©å±•åä½†éœ€è¦åŒ…å«çš„ç‰¹æ®Šæ–‡ä»¶
SPECIAL_FILES = {
    'Makefile', 'Dockerfile', 'Rakefile', 'Gemfile', 
    'Procfile', 'Vagrantfile', 'Jenkinsfile'
}


def xml_escape(text):
    """XMLè½¬ä¹‰ï¼Œå¤„ç†ç‰¹æ®Šå­—ç¬¦"""
    if text is None:
        return ""
    return saxutils.escape(str(text))


def should_include_file(file_path):
    """åˆ¤æ–­æ–‡ä»¶æ˜¯å¦åº”è¯¥è¢«åŒ…å«"""
    file_path = Path(file_path)
    
    # æ’é™¤ç‰¹å®šæ–‡ä»¶
    if file_path.name in EXCLUDE_FILES:
        return False
    
    # åŒ…å«ç‰¹æ®Šæ–‡ä»¶
    if file_path.name in SPECIAL_FILES:
        return True
    
    # æ£€æŸ¥æ‰©å±•å
    return file_path.suffix.lower() in INCLUDE_EXTENSIONS


def generate_tree_node(path, prefix="", is_last=True, parent_chain=None):
    """
    é€’å½’ç”Ÿæˆç›®å½•æ ‘çš„XMLèŠ‚ç‚¹
    
    Args:
        path: å½“å‰è·¯å¾„
        prefix: æ˜¾ç¤ºç”¨çš„å‰ç¼€ï¼ˆç”¨äºæ ‘å½¢ç»“æ„ï¼‰
        is_last: æ˜¯å¦æ˜¯åŒçº§æœ€åä¸€ä¸ª
        parent_chain: çˆ¶è·¯å¾„é“¾ï¼ˆç”¨äºæ£€æµ‹å¾ªç¯ï¼‰
    
    Returns:
        (tree_lines, file_nodes) - æ ‘å½¢ç»“æ„è¡Œåˆ—è¡¨å’Œæ–‡ä»¶èŠ‚ç‚¹åˆ—è¡¨
    """
    if parent_chain is None:
        parent_chain = set()
    
    # é˜²æ­¢å¾ªç¯å¼•ç”¨ï¼ˆç¬¦å·é“¾æ¥ï¼‰
    try:
        real_path = path.resolve()
        if real_path in parent_chain:
            return [], []
    except (OSError, RuntimeError):
        return [], []
    
    tree_lines = []
    file_nodes = []
    
    # è¯»å–ç›®å½•å†…å®¹
    try:
        items = list(path.iterdir())
    except PermissionError:
        return [f"{prefix}[Permission Denied]"], []
    except Exception as e:
        return [f"{prefix}[Error: {str(e)}]"], []
    
    # è¿‡æ»¤å¹¶æ’åºï¼šæ–‡ä»¶å¤¹åœ¨å‰ï¼Œç„¶åæŒ‰åç§°æ’åº
    items = [
        item for item in items
        if item.name not in EXCLUDE_DIRS and item.name not in EXCLUDE_FILES
    ]
    items.sort(key=lambda x: (not x.is_dir(), x.name.lower()))
    
    # æ›´æ–°çˆ¶è·¯å¾„é“¾
    new_chain = parent_chain | {real_path}
    
    # éå†æ‰€æœ‰é¡¹ç›®
    for index, item in enumerate(items):
        is_last_item = (index == len(items) - 1)
        connector = "â””â”€â”€ " if is_last_item else "â”œâ”€â”€ "
        
        if item.is_dir():
            # å¤„ç†æ–‡ä»¶å¤¹
            tree_lines.append(f"{prefix}{connector}{item.name}/")
            
            # é€’å½’å¤„ç†å­ç›®å½•
            extension = "    " if is_last_item else "â”‚   "
            sub_tree, sub_files = generate_tree_node(
                item, 
                prefix + extension, 
                is_last_item,
                new_chain
            )
            tree_lines.extend(sub_tree)
            file_nodes.extend(sub_files)
        else:
            # å¤„ç†æ–‡ä»¶
            tree_lines.append(f"{prefix}{connector}{item.name}")
            
            # å¦‚æœæ˜¯éœ€è¦åŒ…å«çš„ä»£ç æ–‡ä»¶ï¼Œæ·»åŠ åˆ°æ–‡ä»¶åˆ—è¡¨
            if should_include_file(item):
                file_nodes.append(item)
    
    return tree_lines, file_nodes


def read_file_content(file_path):
    """
    è¯»å–æ–‡ä»¶å†…å®¹ï¼Œå°è¯•å¤šç§ç¼–ç 
    
    Returns:
        (content, encoding, error) - æ–‡ä»¶å†…å®¹ã€ä½¿ç”¨çš„ç¼–ç ã€é”™è¯¯ä¿¡æ¯
    """
    encodings = ['utf-8', 'utf-8-sig', 'gbk', 'gb2312', 'gb18030', 'latin-1', 'cp1252']
    
    for encoding in encodings:
        try:
            with open(file_path, 'r', encoding=encoding) as f:
                content = f.read()
                return content, encoding, None
        except UnicodeDecodeError:
            continue
        except Exception as e:
            return None, None, f"Read error: {str(e)}"
    
    return None, None, "Cannot decode with any known encoding"


def get_file_info(file_path, root_path):
    """è·å–æ–‡ä»¶çš„è¯¦ç»†ä¿¡æ¯"""
    try:
        stat = file_path.stat()
        relative_path = file_path.relative_to(root_path)
        
        return {
            'path': str(relative_path),
            'name': file_path.name,
            'extension': file_path.suffix or 'none',
            'size': stat.st_size,
            'modified': datetime.fromtimestamp(stat.st_mtime).isoformat()
        }
    except Exception as e:
        return None


def export_project_xml(root_path=None, output_file=None):
    """
    å¯¼å‡ºé¡¹ç›®ä¸ºXMLæ ¼å¼
    
    Args:
        root_path: é¡¹ç›®æ ¹è·¯å¾„ï¼ˆé»˜è®¤å½“å‰ç›®å½•ï¼‰
        output_file: è¾“å‡ºæ–‡ä»¶å
    """
    if root_path is None:
        root_path = os.getcwd()
    
    if output_file is None:
        output_file = OUTPUT_FILENAME
    
    root = Path(root_path).resolve()
    output_path = root / output_file
    
    print(f"=" * 60)
    print(f"é¡¹ç›®ä»£ç å¯¼å‡º - XMLæ ¼å¼")
    print(f"=" * 60)
    print(f"é¡¹ç›®è·¯å¾„: {root}")
    print(f"è¾“å‡ºæ–‡ä»¶: {output_path}")
    print(f"=" * 60)
    
    # ç”Ÿæˆç›®å½•æ ‘å’Œæ”¶é›†æ–‡ä»¶
    print("\n[1/3] æ‰«æé¡¹ç›®ç»“æ„...")
    tree_lines, code_files = generate_tree_node(root)
    
    # æŒ‰è·¯å¾„æ’åºæ–‡ä»¶
    code_files.sort(key=lambda x: str(x.relative_to(root)).lower())
    
    print(f"âœ“ æ‰¾åˆ° {len(code_files)} ä¸ªä»£ç æ–‡ä»¶")
    
    # ç»Ÿè®¡æ–‡ä»¶ç±»å‹
    stats = {}
    total_size = 0
    for file_path in code_files:
        ext = file_path.suffix or 'no_extension'
        stats[ext] = stats.get(ext, 0) + 1
        try:
            total_size += file_path.stat().st_size
        except:
            pass
    
    # å†™å…¥XMLæ–‡ä»¶
    print(f"\n[2/3] ç”ŸæˆXMLæ–‡æ¡£...")
    
    with open(output_path, 'w', encoding='utf-8') as out:
        # XMLå¤´éƒ¨
        out.write('<?xml version="1.0" encoding="UTF-8"?>\n')
        out.write('<project>\n\n')
        
        # ============ å…ƒæ•°æ®éƒ¨åˆ† ============
        out.write('  <!-- é¡¹ç›®å…ƒæ•°æ® -->\n')
        out.write('  <metadata>\n')
        out.write(f'    <name>{xml_escape(root.name)}</name>\n')
        out.write(f'    <path>{xml_escape(str(root))}</path>\n')
        out.write(f'    <export_time>{datetime.now().isoformat()}</export_time>\n')
        out.write(f'    <total_files>{len(code_files)}</total_files>\n')
        out.write(f'    <total_size_bytes>{total_size}</total_size_bytes>\n')
        out.write(f'    <total_size_mb>{total_size / (1024 * 1024):.2f}</total_size_mb>\n')
        out.write('    <file_types>\n')
        
        for ext, count in sorted(stats.items(), key=lambda x: (-x[1], x[0])):
            out.write(f'      <type extension="{xml_escape(ext)}" count="{count}"/>\n')
        
        out.write('    </file_types>\n')
        out.write('  </metadata>\n\n')
        
        # ============ ç›®å½•ç»“æ„éƒ¨åˆ† ============
        out.write('  <!-- ç›®å½•ç»“æ„ -->\n')
        out.write('  <directory_structure>\n')
        out.write('    <tree><![CDATA[\n')
        out.write(f'{root.name}/\n')
        out.write('\n'.join(tree_lines))
        out.write('\n    ]]></tree>\n')
        out.write('  </directory_structure>\n\n')
        
        # ============ æ–‡ä»¶å†…å®¹éƒ¨åˆ† ============
        out.write('  <!-- æºä»£ç æ–‡ä»¶ -->\n')
        out.write('  <files>\n')
        
        print(f"\n[3/3] è¯»å–æ–‡ä»¶å†…å®¹...")
        
        for index, file_path in enumerate(code_files, 1):
            relative_path = file_path.relative_to(root)
            print(f"  [{index}/{len(code_files)}] {relative_path}")
            
            # è·å–æ–‡ä»¶ä¿¡æ¯
            file_info = get_file_info(file_path, root)
            if not file_info:
                print(f"    âš  æ— æ³•è·å–æ–‡ä»¶ä¿¡æ¯")
                continue
            
            # è¯»å–æ–‡ä»¶å†…å®¹
            content, encoding, error = read_file_content(file_path)
            
            # å†™å…¥æ–‡ä»¶èŠ‚ç‚¹
            out.write(f'\n    <file index="{index}">\n')
            out.write(f'      <path>{xml_escape(file_info["path"])}</path>\n')
            out.write(f'      <name>{xml_escape(file_info["name"])}</name>\n')
            out.write(f'      <extension>{xml_escape(file_info["extension"])}</extension>\n')
            out.write(f'      <size_bytes>{file_info["size"]}</size_bytes>\n')
            out.write(f'      <modified>{file_info["modified"]}</modified>\n')
            
            if error:
                out.write(f'      <error>{xml_escape(error)}</error>\n')
                out.write('      <content/>\n')
                print(f"    âš  è¯»å–å¤±è´¥: {error}")
            else:
                out.write(f'      <encoding>{encoding}</encoding>\n')
                out.write(f'      <lines>{len(content.splitlines())}</lines>\n')
                out.write('      <content><![CDATA[\n')
                out.write(content)
                if not content.endswith('\n'):
                    out.write('\n')
                out.write(']]></content>\n')
            
            out.write('    </file>\n')
        
        out.write('\n  </files>\n\n')
        out.write('</project>\n')
    
    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    print(f"\n{'=' * 60}")
    print(f"âœ“ å¯¼å‡ºå®Œæˆ!")
    print(f"{'=' * 60}")
    print(f"è¾“å‡ºæ–‡ä»¶: {output_path}")
    print(f"æ–‡ä»¶å¤§å°: {output_path.stat().st_size / 1024:.2f} KB")
    print(f"æ€»è®¡æ–‡ä»¶: {len(code_files)} ä¸ª")
    print(f"ä»£ç æ€»é‡: {total_size / 1024:.2f} KB")
    print(f"\næ–‡ä»¶ç±»å‹ç»Ÿè®¡:")
    for ext, count in sorted(stats.items(), key=lambda x: (-x[1], x[0]))[:10]:
        print(f"  {ext:15s} : {count:3d} ä¸ª")
    print(f"{'=' * 60}\n")


if __name__ == "__main__":
    # å¯¼å‡ºå½“å‰ç›®å½•
    export_project_xml()
    
    # æˆ–æŒ‡å®šå…¶ä»–ç›®å½•
    # export_project_xml(root_path="/path/to/your/project")
    
    # æˆ–æŒ‡å®šè¾“å‡ºæ–‡ä»¶å
    # export_project_xml(output_file="my_project.xml")
]]></content>
    </file>

    <file index="8">
      <path>pybind/pybind_wrapper.cpp</path>
      <name>pybind_wrapper.cpp</name>
      <extension>.cpp</extension>
      <size_bytes>6628</size_bytes>
      <modified>2026-01-17T17:29:00.482236</modified>
      <encoding>utf-8</encoding>
      <lines>183</lines>
      <content><![CDATA[
#include <pybind11/pybind11.h>
#include <pybind11/stl.h>
#include <pybind11/numpy.h> 
#include <cuda_runtime.h>
#include <cublas_v2.h>
#include <cstring>
#include <vector>
#include <stdexcept>

#include "tensor.h"
#include "layers.h"
#include "conv_layer.h"  
#include "elementwise.h"

namespace py = pybind11;

// è¾…åŠ©å‡½æ•°ï¼šä» numpy array åˆ›å»º Tensor
Tensor from_numpy(py::array_t<float> array, Device device) {
    py::buffer_info buf = array.request();
    
    std::vector<int64_t> shape;
    for (auto s : buf.shape) {
        shape.push_back(static_cast<int64_t>(s));
    }
    
    Tensor t(shape, device);
    
    if (device.type == DeviceType::CPU) {
        std::memcpy(t.data(), buf.ptr, buf.size * sizeof(float));
    } else {
        cudaMemcpy(t.data(), buf.ptr, buf.size * sizeof(float), cudaMemcpyHostToDevice);
    }
    return t;
}

// è¾…åŠ©å‡½æ•°ï¼šTensor è½¬ Numpy
py::array_t<float> to_numpy(const Tensor& t) {
    std::vector<int64_t> shape = t.shape();
    std::vector<ssize_t> strides;
    ssize_t stride = sizeof(float);
    
    for (int i = shape.size() - 1; i >= 0; --i) {
        strides.insert(strides.begin(), stride);
        stride *= shape[i];
    }

    py::array_t<float> result(shape, strides);
    py::buffer_info buf = result.request();

    Tensor cpu_t = t.cpu(); 
    std::memcpy(buf.ptr, cpu_t.data(), cpu_t.numel() * sizeof(float));
    
    return result;
}

PYBIND11_MODULE(my_deep_lib, m) {
    m.doc() = "Deep Learning Library with CUDA Backend";

    // 1. ç»‘å®šæšä¸¾
    py::enum_<DeviceType>(m, "DeviceType")
        .value("CPU", DeviceType::CPU)
        .value("GPU", DeviceType::GPU)
        .export_values();

    // 2. ç»‘å®š Device ç±»
    py::class_<Device>(m, "Device")
        .def(py::init<DeviceType, int>(), py::arg("type"), py::arg("index")=0)
        .def_static("cpu", &Device::cpu)
        .def_static("gpu", &Device::gpu)
        .def_readonly("type", &Device::type)
        .def_readonly("index", &Device::index);

    // 3. ç»‘å®š Tensor ç±»
    py::class_<Tensor>(m, "Tensor")
        .def(py::init<const std::vector<int64_t>&, Device>())
        .def_static("from_numpy", &from_numpy)
        .def("to_numpy", &to_numpy)
        .def("fill", &Tensor::fill)
        .def("shape", &Tensor::shape)
        .def("numel", &Tensor::numel)
        .def("cpu", &Tensor::cpu)
        .def("gpu", &Tensor::gpu, py::arg("index")=0);

    // 4. æ¿€æ´»å‡½æ•°
    m.def("sigmoid_forward", &sigmoid_forward, 
          "Sigmoid forward pass");
    m.def("sigmoid_backward", &sigmoid_backward,
          "Sigmoid backward pass");
    
    m.def("relu_forward", &relu_forward,
          "ReLU forward pass");
    m.def("relu_backward", &relu_backward,
          "ReLU backward pass");
    
    // 5. å…¨è¿æ¥å±‚
    m.def("linear_forward", &linear_forward,
          "Linear layer forward pass");
    m.def("linear_backward", &linear_backward,
          "Linear layer backward pass");
    
    // 6. å·ç§¯å±‚ï¼ˆå‰å‘å’Œåå‘ï¼‰
    m.def("conv2d_forward", &conv2d_forward,
          py::arg("input"), py::arg("weight"), py::arg("output"),
          py::arg("stride"), py::arg("padding"),
          "Convolution forward pass");
    
    // ğŸ”´ æ–°å¢ï¼šå·ç§¯åå‘ä¼ æ’­
    m.def("conv2d_backward", [](py::array_t<float> input, 
                                 py::array_t<float> grad_output,
                                 py::array_t<float> weight,
                                 int batch, int in_c, int out_c, 
                                 int h, int w, int ksize, int stride, int padding) {
        // è½¬æ¢ä¸º CUDA Tensor
        Tensor d_input = from_numpy(input, Device::gpu(0));
        Tensor d_grad_output = from_numpy(grad_output, Device::gpu(0));
        Tensor d_weight = from_numpy(weight, Device::gpu(0));
        
        // åˆ†é…æ¢¯åº¦å¼ é‡
        Tensor d_grad_input(d_input.shape(), Device::gpu(0));
        Tensor d_grad_weight(d_weight.shape(), Device::gpu(0));
        
        // è®¡ç®—è¾“å‡ºå°ºå¯¸å¹¶åˆ†é… col_buffer
        int h_out = (h + 2*padding - ksize) / stride + 1;
        int w_out = (w + 2*padding - ksize) / stride + 1;
        std::vector<int64_t> col_shape = {in_c * ksize * ksize, h_out * w_out};
        Tensor col_buffer(col_shape, Device::gpu(0));
        
        // åˆ›å»º cuBLAS handle
        cublasHandle_t handle;
        cublasCreate(&handle);
        
        // è°ƒç”¨ CUDA backward kernel
        backward_conv(d_input.data(), d_grad_output.data(), d_weight.data(),
                     col_buffer.data(), d_grad_input.data(), d_grad_weight.data(),
                     batch, in_c, out_c, h, w, ksize, stride, padding, handle);
        
        cublasDestroy(handle);
        
        // è½¬å› NumPy
        return py::make_tuple(to_numpy(d_grad_input), to_numpy(d_grad_weight));
    }, py::arg("input"), py::arg("grad_output"), py::arg("weight"),
       py::arg("batch"), py::arg("in_c"), py::arg("out_c"),
       py::arg("h"), py::arg("w"), py::arg("ksize"), 
       py::arg("stride"), py::arg("padding"),
       "Convolution backward pass");
    
    // 7. æ± åŒ–å±‚
    m.def("maxpool_forward", &maxpool_forward,
          py::arg("input"), py::arg("output"), py::arg("mask"),
          py::arg("k"), py::arg("s"), py::arg("p"),
          "Max pooling forward pass");
    m.def("maxpool_backward", &maxpool_backward,
          py::arg("grad_output"), py::arg("mask"), py::arg("grad_input"),
          py::arg("k"), py::arg("s"), py::arg("p"),
          "Max pooling backward pass");
    
    // 8. Softmax å’ŒæŸå¤±å‡½æ•°
    m.def("softmax_forward", &softmax_forward,
          "Softmax forward pass");
    
    m.def("cross_entropy_loss", &cross_entropy_loss,
          "Cross entropy loss computation");
    m.def("cross_entropy_backward", &cross_entropy_backward,
          "Cross entropy backward pass");
    
    m.def("elementwise_add", [](py::array_t<float> a, py::array_t<float> b) {
        Tensor ta = from_numpy(a, Device::gpu(0));
        Tensor tb = from_numpy(b, Device::gpu(0));
        Tensor tc(ta.shape(), Device::gpu(0));
        elementwise_add(ta, tb, tc);
        return to_numpy(tc);
    }, py::arg("a"), py::arg("b"),
       "Element-wise addition");
    
    m.def("elementwise_add_backward", [](py::array_t<float> grad_out) {
        Tensor tg = from_numpy(grad_out, Device::gpu(0));
        Tensor ga(tg.shape(), Device::gpu(0));
        Tensor gb(tg.shape(), Device::gpu(0));
        elementwise_add_backward(tg, ga, gb);
        return py::make_tuple(to_numpy(ga), to_numpy(gb));
    }, py::arg("grad_out"),
       "Element-wise addition backward");
}
]]></content>
    </file>

    <file index="9">
      <path>python/core/device.py</path>
      <name>device.py</name>
      <extension>.py</extension>
      <size_bytes>495</size_bytes>
      <modified>2026-01-17T17:21:37.118364</modified>
      <encoding>utf-8</encoding>
      <lines>20</lines>
      <content><![CDATA[
class Device:
    def __init__(self, device_type, device_id=0):
        self.device_type = device_type  # "cpu" or "cuda"
        self.device_id = device_id
        
    @staticmethod
    def cpu():
        return Device("cpu", 0)
    
    @staticmethod
    def cuda(device_id=0):
        return Device("cuda", device_id)
    
    def __repr__(self):
        if self.device_type == "cpu":
            return "cpu()"
        return f"cuda({self.device_id})"

cpu = Device.cpu()
cuda = Device.cuda
]]></content>
    </file>

    <file index="10">
      <path>python/cuda_ops/__init__.py</path>
      <name>__init__.py</name>
      <extension>.py</extension>
      <size_bytes>267</size_bytes>
      <modified>2026-01-17T17:41:26.402040</modified>
      <encoding>utf-8</encoding>
      <lines>16</lines>
      <content><![CDATA[
"""
CUDA ç®—å­çš„ç»Ÿä¸€å¯¼å‡º
"""
from .conv import conv2d
from .pooling import maxpool2d
from .linear import linear
from .activations import relu, softmax, sigmoid

__all__ = [
    'conv2d',
    'maxpool2d',
    'linear',
    'relu',
    'softmax',
    'sigmoid',
]
]]></content>
    </file>

    <file index="11">
      <path>python/cuda_ops/activations.py</path>
      <name>activations.py</name>
      <extension>.py</extension>
      <size_bytes>4560</size_bytes>
      <modified>2026-01-17T17:33:31.454168</modified>
      <encoding>utf-8</encoding>
      <lines>119</lines>
      <content><![CDATA[
"""
æ¿€æ´»å‡½æ•°çš„ CUDA ç®—å­åŒ…è£…
"""
import numpy as np
from typing import Tuple
import my_deep_lib as cuda_lib
from basic_operator import Op, Value

class ReLUOp(Op):
    """ReLU æ¿€æ´»å‡½æ•°"""
    
    def compute(self, input_data: np.ndarray) -> np.ndarray:
        """
        å‰å‘ä¼ æ’­ï¼šy = max(0, x)
        """
        # è½¬æ¢ä¸º CUDA Tensor
        cuda_input = cuda_lib.Tensor.from_numpy(input_data, cuda_lib.Device.gpu(0))
        cuda_output = cuda_lib.Tensor(list(input_data.shape), cuda_lib.Device.gpu(0))
        
        # è°ƒç”¨ CUDA kernel
        cuda_lib.relu_forward(cuda_input, cuda_output)
        
        return cuda_output.to_numpy()
    
    def gradient(self, out_grad: Value, node: Value) -> Tuple[Value]:
        """
        åå‘ä¼ æ’­ï¼šgrad_input = grad_output * (input > 0)
        """
        input_node = node.inputs[0]
        input_data = input_node.realize_cached_data()
        grad_output_data = out_grad.realize_cached_data()
        
        # è½¬æ¢ä¸º CUDA Tensor
        cuda_input = cuda_lib.Tensor.from_numpy(input_data, cuda_lib.Device.gpu(0))
        cuda_grad_output = cuda_lib.Tensor.from_numpy(grad_output_data, cuda_lib.Device.gpu(0))
        cuda_grad_input = cuda_lib.Tensor(list(input_data.shape), cuda_lib.Device.gpu(0))
        
        # è°ƒç”¨ CUDA backward kernel
        cuda_lib.relu_backward(cuda_input, cuda_grad_output, cuda_grad_input)
        
        grad_input_np = cuda_grad_input.to_numpy()
        grad_input = Value.make_const(grad_input_np, requires_grad=False)
        
        return (grad_input,)

class SoftmaxOp(Op):
    """Softmax æ¿€æ´»å‡½æ•°ï¼ˆé€šå¸¸ç”¨äºè¾“å‡ºå±‚ï¼‰"""
    
    def compute(self, input_data: np.ndarray) -> np.ndarray:
        """
        å‰å‘ä¼ æ’­ï¼šsoftmax(x) = exp(x) / sum(exp(x))
        input_data: [batch, num_classes]
        """
        # è½¬æ¢ä¸º CUDA Tensor
        cuda_input = cuda_lib.Tensor.from_numpy(input_data, cuda_lib.Device.gpu(0))
        cuda_output = cuda_lib.Tensor(list(input_data.shape), cuda_lib.Device.gpu(0))
        
        # è°ƒç”¨ CUDA kernel
        cuda_lib.softmax_forward(cuda_input, cuda_output)
        
        return cuda_output.to_numpy()
    
    def gradient(self, out_grad: Value, node: Value) -> Tuple[Value]:
        """
        Softmax çš„æ¢¯åº¦æ¯”è¾ƒå¤æ‚ï¼Œä½†é€šå¸¸ä¸ CrossEntropy èåˆè®¡ç®—
        è¿™é‡Œæä¾›ä¸€ä¸ªç®€åŒ–ç‰ˆæœ¬
        """
        # è·å– softmax çš„è¾“å‡º
        output_data = node.realize_cached_data()
        grad_output_data = out_grad.realize_cached_data()
        
        # Softmax æ¢¯åº¦ï¼šgrad_input = softmax * (grad_output - sum(grad_output * softmax))
        # è¿™é‡Œç”¨ NumPy å®ç°ï¼ˆå› ä¸º CUDA kernel é‡Œæ²¡æœ‰å•ç‹¬çš„ softmax backwardï¼‰
        sum_term = np.sum(grad_output_data * output_data, axis=1, keepdims=True)
        grad_input_np = output_data * (grad_output_data - sum_term)
        
        grad_input = Value.make_const(grad_input_np, requires_grad=False)
        return (grad_input,)

class SigmoidOp(Op):
    """Sigmoid æ¿€æ´»å‡½æ•°"""
    
    def compute(self, input_data: np.ndarray) -> np.ndarray:
        """å‰å‘ä¼ æ’­ï¼šy = 1 / (1 + exp(-x))"""
        cuda_input = cuda_lib.Tensor.from_numpy(input_data, cuda_lib.Device.gpu(0))
        cuda_output = cuda_lib.Tensor(list(input_data.shape), cuda_lib.Device.gpu(0))
        
        cuda_lib.sigmoid_forward(cuda_input, cuda_output)
        
        return cuda_output.to_numpy()
    
    def gradient(self, out_grad: Value, node: Value) -> Tuple[Value]:
        """åå‘ä¼ æ’­ï¼šgrad_input = grad_output * y * (1 - y)"""
        output_data = node.realize_cached_data()  # sigmoid çš„è¾“å‡º
        grad_output_data = out_grad.realize_cached_data()
        
        cuda_output = cuda_lib.Tensor.from_numpy(output_data, cuda_lib.Device.gpu(0))
        cuda_grad_output = cuda_lib.Tensor.from_numpy(grad_output_data, cuda_lib.Device.gpu(0))
        cuda_grad_input = cuda_lib.Tensor(list(output_data.shape), cuda_lib.Device.gpu(0))
        
        cuda_lib.sigmoid_backward(cuda_output, cuda_grad_output, cuda_grad_input)
        
        grad_input_np = cuda_grad_input.to_numpy()
        grad_input = Value.make_const(grad_input_np, requires_grad=False)
        
        return (grad_input,)

# é«˜å±‚æ¥å£
def relu(x: Value) -> Value:
    """ReLU æ¿€æ´»å‡½æ•°"""
    return ReLUOp()(x)

def softmax(x: Value) -> Value:
    """Softmax æ¿€æ´»å‡½æ•°"""
    return SoftmaxOp()(x)

def sigmoid(x: Value) -> Value:
    """Sigmoid æ¿€æ´»å‡½æ•°"""
    return SigmoidOp()(x)
]]></content>
    </file>

    <file index="12">
      <path>python/cuda_ops/conv.py</path>
      <name>conv.py</name>
      <extension>.py</extension>
      <size_bytes>2687</size_bytes>
      <modified>2026-01-17T17:28:40.634241</modified>
      <encoding>utf-8</encoding>
      <lines>76</lines>
      <content><![CDATA[
"""
CUDA Conv2D ç®—å­çš„ Python åŒ…è£…
"""
import numpy as np
from typing import Tuple
import my_deep_lib as cuda_lib
from basic_operator import Op, Value

class Conv2DOp(Op):
    def __init__(self, stride=1, padding=0):
        self.stride = stride
        self.padding = padding
    
    def compute(self, input_data: np.ndarray, weight_data: np.ndarray) -> np.ndarray:
        """
        input_data: [batch, in_channels, height, width]
        weight_data: [out_channels, in_channels, kernel_h, kernel_w]
        """
        batch, in_c, in_h, in_w = input_data.shape
        out_c, _, k_h, k_w = weight_data.shape
        
        # è®¡ç®—è¾“å‡ºå°ºå¯¸
        out_h = (in_h + 2 * self.padding - k_h) // self.stride + 1
        out_w = (in_w + 2 * self.padding - k_w) // self.stride + 1
        
        # è½¬æ¢ä¸º CUDA Tensor
        cuda_input = cuda_lib.Tensor.from_numpy(input_data, cuda_lib.Device.gpu(0))
        cuda_weight = cuda_lib.Tensor.from_numpy(weight_data, cuda_lib.Device.gpu(0))
        cuda_output = cuda_lib.Tensor([batch, out_c, out_h, out_w], cuda_lib.Device.gpu(0))
        
        # è°ƒç”¨ CUDA kernel
        cuda_lib.conv2d_forward(cuda_input, cuda_weight, cuda_output, 
                               self.stride, self.padding)
        
        # è½¬å› NumPy
        return cuda_output.to_numpy()
    
    def gradient(self, out_grad: Value, node: Value) -> Tuple[Value, Value]:
        """
        è®¡ç®—è¾“å…¥å’Œæƒé‡çš„æ¢¯åº¦
        """
        input_node, weight_node = node.inputs
        input_data = input_node.realize_cached_data()
        weight_data = weight_node.realize_cached_data()
        grad_output_data = out_grad.realize_cached_data()
        
        batch, in_c, in_h, in_w = input_data.shape
        out_c, _, k_h, k_w = weight_data.shape
        
        # è°ƒç”¨ CUDA backward
        grad_input_np, grad_weight_np = cuda_lib.conv2d_backward(
            input_data, grad_output_data, weight_data,
            batch, in_c, out_c, in_h, in_w, k_h, 
            self.stride, self.padding
        )
        
        # åŒ…è£…ä¸º Value
        grad_input = Value.make_const(grad_input_np, requires_grad=False)
        grad_weight = Value.make_const(grad_weight_np, requires_grad=False)
        
        return grad_input, grad_weight

def conv2d(input: Value, weight: Value, stride=1, padding=0) -> Value:
    """
    å·ç§¯æ“ä½œçš„é«˜å±‚æ¥å£
    
    Args:
        input: [batch, in_channels, H, W]
        weight: [out_channels, in_channels, kH, kW]
        stride: æ­¥é•¿
        padding: å¡«å……
    
    Returns:
        output: [batch, out_channels, H_out, W_out]
    """
    return Conv2DOp(stride, padding)(input, weight)
]]></content>
    </file>

    <file index="13">
      <path>python/cuda_ops/linear.py</path>
      <name>linear.py</name>
      <extension>.py</extension>
      <size_bytes>3230</size_bytes>
      <modified>2026-01-17T17:39:48.470061</modified>
      <encoding>utf-8</encoding>
      <lines>81</lines>
      <content><![CDATA[
"""
å…¨è¿æ¥å±‚çš„ CUDA ç®—å­åŒ…è£…
"""
import numpy as np
from typing import Tuple
import my_deep_lib as cuda_lib
from basic_operator import Op, Value

class LinearOp(Op):
    """å…¨è¿æ¥å±‚ï¼ˆçŸ©é˜µä¹˜æ³• + biasï¼‰"""
    
    def compute(self, input_data: np.ndarray, weight_data: np.ndarray, 
                bias_data: np.ndarray) -> np.ndarray:
        """
        å‰å‘ä¼ æ’­ï¼šy = x @ W^T + b
        input_data: [batch, in_features]
        weight_data: [out_features, in_features]
        bias_data: [out_features]
        """
        batch_size = input_data.shape[0]
        out_features = weight_data.shape[0]
        
        # è½¬æ¢ä¸º CUDA Tensor
        cuda_input = cuda_lib.Tensor.from_numpy(input_data, cuda_lib.Device.gpu(0))
        cuda_weight = cuda_lib.Tensor.from_numpy(weight_data, cuda_lib.Device.gpu(0))
        cuda_bias = cuda_lib.Tensor.from_numpy(bias_data, cuda_lib.Device.gpu(0))
        cuda_output = cuda_lib.Tensor([batch_size, out_features], cuda_lib.Device.gpu(0))
        
        # è°ƒç”¨ CUDA kernel
        cuda_lib.linear_forward(cuda_input, cuda_weight, cuda_bias, cuda_output)
        
        return cuda_output.to_numpy()
    
    def gradient(self, out_grad: Value, node: Value) -> Tuple[Value, Value, Value]:
        """
        åå‘ä¼ æ’­
        grad_input = grad_output @ W
        grad_weight = grad_output^T @ input
        grad_bias = sum(grad_output, axis=0)
        """
        input_node, weight_node, bias_node = node.inputs
        input_data = input_node.realize_cached_data()
        weight_data = weight_node.realize_cached_data()
        grad_output_data = out_grad.realize_cached_data()
        
        # è½¬æ¢ä¸º CUDA Tensor
        cuda_input = cuda_lib.Tensor.from_numpy(input_data, cuda_lib.Device.gpu(0))
        cuda_weight = cuda_lib.Tensor.from_numpy(weight_data, cuda_lib.Device.gpu(0))
        cuda_grad_output = cuda_lib.Tensor.from_numpy(grad_output_data, cuda_lib.Device.gpu(0))
        
        cuda_grad_input = cuda_lib.Tensor(list(input_data.shape), cuda_lib.Device.gpu(0))
        cuda_grad_weight = cuda_lib.Tensor(list(weight_data.shape), cuda_lib.Device.gpu(0))
        cuda_grad_bias = cuda_lib.Tensor([weight_data.shape[0]], cuda_lib.Device.gpu(0))
        
        # è°ƒç”¨ CUDA backward kernel
        cuda_lib.linear_backward(cuda_input, cuda_weight, cuda_grad_output,
                                cuda_grad_input, cuda_grad_weight, cuda_grad_bias)
        
        grad_input_np = cuda_grad_input.to_numpy()
        grad_weight_np = cuda_grad_weight.to_numpy()
        grad_bias_np = cuda_grad_bias.to_numpy()
        
        grad_input = Value.make_const(grad_input_np, requires_grad=False)
        grad_weight = Value.make_const(grad_weight_np, requires_grad=False)
        grad_bias = Value.make_const(grad_bias_np, requires_grad=False)
        
        return grad_input, grad_weight, grad_bias

def linear(input: Value, weight: Value, bias: Value) -> Value:
    """
    å…¨è¿æ¥å±‚æ“ä½œ
    
    Args:
        input: [batch, in_features]
        weight: [out_features, in_features]
        bias: [out_features]
    
    Returns:
        output: [batch, out_features]
    """
    return LinearOp()(input, weight, bias)
]]></content>
    </file>

    <file index="14">
      <path>python/cuda_ops/pooling.py</path>
      <name>pooling.py</name>
      <extension>.py</extension>
      <size_bytes>2882</size_bytes>
      <modified>2026-01-17T17:28:58.414236</modified>
      <encoding>utf-8</encoding>
      <lines>75</lines>
      <content><![CDATA[
"""
CUDA MaxPool2D ç®—å­çš„ Python åŒ…è£…
"""
import numpy as np
from typing import Tuple
import my_deep_lib as cuda_lib
from basic_operator import Op, Value

class MaxPool2DOp(Op):
    def __init__(self, kernel_size, stride=None, padding=0):
        self.kernel_size = kernel_size
        self.stride = stride if stride is not None else kernel_size
        self.padding = padding
        self.mask = None  # å­˜å‚¨ forward æ—¶çš„ maskï¼Œç”¨äº backward
    
    def compute(self, input_data: np.ndarray) -> np.ndarray:
        """
        input_data: [batch, channels, height, width]
        """
        batch, channels, in_h, in_w = input_data.shape
        
        # è®¡ç®—è¾“å‡ºå°ºå¯¸
        out_h = (in_h + 2 * self.padding - self.kernel_size) // self.stride + 1
        out_w = (in_w + 2 * self.padding - self.kernel_size) // self.stride + 1
        
        # è½¬æ¢ä¸º CUDA Tensor
        cuda_input = cuda_lib.Tensor.from_numpy(input_data, cuda_lib.Device.gpu(0))
        cuda_output = cuda_lib.Tensor([batch, channels, out_h, out_w], cuda_lib.Device.gpu(0))
        cuda_mask = cuda_lib.Tensor([batch, channels, out_h, out_w], cuda_lib.Device.gpu(0))
        
        # è°ƒç”¨ CUDA kernel
        cuda_lib.maxpool_forward(cuda_input, cuda_output, cuda_mask,
                                self.kernel_size, self.stride, self.padding)
        
        # ä¿å­˜ mask ç”¨äº backwardï¼ˆè½¬ä¸º NumPy ä¿å­˜ï¼‰
        self.mask = cuda_mask.to_numpy()
        
        return cuda_output.to_numpy()
    
    def gradient(self, out_grad: Value, node: Value) -> Tuple[Value]:
        """
        MaxPool çš„æ¢¯åº¦ä¼ æ’­
        """
        input_node = node.inputs[0]
        input_data = input_node.realize_cached_data()
        grad_output_data = out_grad.realize_cached_data()
        
        # è½¬æ¢ä¸º CUDA Tensor
        cuda_grad_output = cuda_lib.Tensor.from_numpy(grad_output_data, cuda_lib.Device.gpu(0))
        cuda_mask = cuda_lib.Tensor.from_numpy(self.mask, cuda_lib.Device.gpu(0))
        cuda_grad_input = cuda_lib.Tensor(list(input_data.shape), cuda_lib.Device.gpu(0))
        
        # è°ƒç”¨ CUDA backward kernel
        cuda_lib.maxpool_backward(cuda_grad_output, cuda_mask, cuda_grad_input,
                                 self.kernel_size, self.stride, self.padding)
        
        grad_input_np = cuda_grad_input.to_numpy()
        grad_input = Value.make_const(grad_input_np, requires_grad=False)
        
        return (grad_input,)

def maxpool2d(input: Value, kernel_size, stride=None, padding=0) -> Value:
    """
    MaxPool2D æ“ä½œ
    
    Args:
        input: [batch, channels, H, W]
        kernel_size: æ± åŒ–çª—å£å¤§å°
        stride: æ­¥é•¿ï¼ˆé»˜è®¤ç­‰äº kernel_sizeï¼‰
        padding: å¡«å……
    
    Returns:
        output: [batch, channels, H_out, W_out]
    """
    return MaxPool2DOp(kernel_size, stride, padding)(input)
]]></content>
    </file>

    <file index="15">
      <path>python/nn/__init__.py</path>
      <name>__init__.py</name>
      <extension>.py</extension>
      <size_bytes>386</size_bytes>
      <modified>2026-01-17T17:41:39.354036</modified>
      <encoding>utf-8</encoding>
      <lines>20</lines>
      <content><![CDATA[
"""
ç¥ç»ç½‘ç»œæ¨¡å—çš„ç»Ÿä¸€å¯¼å‡º
"""
from .module import Module, Parameter
from .conv import Conv2D, MaxPool2D
from .linear import Linear
from .activations import ReLU, Softmax, Sigmoid
from .loss import cross_entropy_loss

__all__ = [
    'Module',
    'Parameter',
    'Conv2D',
    'MaxPool2D',
    'Linear',
    'ReLU',
    'Softmax',
    'Sigmoid',
    'cross_entropy_loss',
]
]]></content>
    </file>

    <file index="16">
      <path>python/nn/activations.py</path>
      <name>activations.py</name>
      <extension>.py</extension>
      <size_bytes>740</size_bytes>
      <modified>2026-01-17T17:40:24.062056</modified>
      <encoding>utf-8</encoding>
      <lines>31</lines>
      <content><![CDATA[
"""
æ¿€æ´»å‡½æ•°å±‚çš„å°è£…
"""
from nn.module import Module
from basic_operator import Value
from cuda_ops.activations import relu, softmax, sigmoid

class ReLU(Module):
    """ReLU æ¿€æ´»å±‚"""
    def __init__(self):
        super().__init__()
    
    def forward(self, x: Value) -> Value:
        return relu(x)

class Softmax(Module):
    """Softmax æ¿€æ´»å±‚"""
    def __init__(self, dim=-1):
        super().__init__()
        self.dim = dim  # æš‚æ—¶ä¸ç”¨ï¼Œé»˜è®¤åœ¨æœ€åä¸€ç»´
    
    def forward(self, x: Value) -> Value:
        return softmax(x)

class Sigmoid(Module):
    """Sigmoid æ¿€æ´»å±‚"""
    def __init__(self):
        super().__init__()
    
    def forward(self, x: Value) -> Value:
        return sigmoid(x)
]]></content>
    </file>

    <file index="17">
      <path>python/nn/conv.py</path>
      <name>conv.py</name>
      <extension>.py</extension>
      <size_bytes>2177</size_bytes>
      <modified>2026-01-17T17:29:48.126225</modified>
      <encoding>utf-8</encoding>
      <lines>73</lines>
      <content><![CDATA[
"""
å·ç§¯å’Œæ± åŒ–å±‚çš„å°è£…
"""
import numpy as np
import math
from nn.module import Module, Parameter
from basic_operator import Value
from cuda_ops.conv import conv2d
from cuda_ops.pooling import maxpool2d

class Conv2D(Module):
    """
    2D å·ç§¯å±‚
    
    Args:
        in_channels: è¾“å…¥é€šé“æ•°
        out_channels: è¾“å‡ºé€šé“æ•°
        kernel_size: å·ç§¯æ ¸å¤§å°
        stride: æ­¥é•¿
        padding: å¡«å……
    """
    def __init__(self, in_channels, out_channels, kernel_size, 
                 stride=1, padding=0, bias=False):
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding
        
        # Kaiming åˆå§‹åŒ–
        fan_in = in_channels * kernel_size * kernel_size
        std = math.sqrt(2.0 / fan_in)
        weight_data = np.random.randn(out_channels, in_channels, 
                                      kernel_size, kernel_size).astype(np.float32) * std
        
        self.weight = Parameter.make_const(weight_data, requires_grad=True)
        
        # ç®€åŒ–ç‰ˆï¼šä¸ä½¿ç”¨ bias
        self.bias = None
    
    def forward(self, x: Value) -> Value:
        """
        x: [batch, in_channels, H, W]
        returns: [batch, out_channels, H_out, W_out]
        """
        out = conv2d(x, self.weight, self.stride, self.padding)
        if self.bias is not None:
            # TODO: æ·»åŠ  biasï¼ˆéœ€è¦å®ç° broadcast addï¼‰
            pass
        return out

class MaxPool2D(Module):
    """
    2D æœ€å¤§æ± åŒ–å±‚
    
    Args:
        kernel_size: æ± åŒ–çª—å£å¤§å°
        stride: æ­¥é•¿ï¼ˆé»˜è®¤ç­‰äº kernel_sizeï¼‰
        padding: å¡«å……
    """
    def __init__(self, kernel_size, stride=None, padding=0):
        super().__init__()
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding
    
    def forward(self, x: Value) -> Value:
        """
        x: [batch, channels, H, W]
        returns: [batch, channels, H_out, W_out]
        """
        return maxpool2d(x, self.kernel_size, self.stride, self.padding)
]]></content>
    </file>

    <file index="18">
      <path>python/nn/linear.py</path>
      <name>linear.py</name>
      <extension>.py</extension>
      <size_bytes>1457</size_bytes>
      <modified>2026-01-17T17:40:08.518058</modified>
      <encoding>utf-8</encoding>
      <lines>46</lines>
      <content><![CDATA[
"""
å…¨è¿æ¥å±‚çš„å°è£…
"""
import numpy as np
import math
from nn.module import Module, Parameter
from basic_operator import Value
from cuda_ops.linear import linear

class Linear(Module):
    """
    å…¨è¿æ¥å±‚
    
    Args:
        in_features: è¾“å…¥ç‰¹å¾æ•°
        out_features: è¾“å‡ºç‰¹å¾æ•°
        bias: æ˜¯å¦ä½¿ç”¨åç½®
    """
    def __init__(self, in_features, out_features, bias=True):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        
        # Kaiming åˆå§‹åŒ–
        std = math.sqrt(1.0 / in_features)
        weight_data = np.random.randn(out_features, in_features).astype(np.float32) * std
        self.weight = Parameter.make_const(weight_data, requires_grad=True)
        
        if bias:
            bias_data = np.zeros(out_features, dtype=np.float32)
            self.bias = Parameter.make_const(bias_data, requires_grad=True)
        else:
            self.bias = None
    
    def forward(self, x: Value) -> Value:
        """
        x: [batch, in_features]
        returns: [batch, out_features]
        """
        if self.bias is not None:
            return linear(x, self.weight, self.bias)
        else:
            # å¦‚æœæ²¡æœ‰ biasï¼Œåˆ›å»ºä¸€ä¸ªé›¶å‘é‡
            bias_data = np.zeros(self.out_features, dtype=np.float32)
            bias = Value.make_const(bias_data, requires_grad=False)
            return linear(x, self.weight, bias)
]]></content>
    </file>

    <file index="19">
      <path>python/nn/loss.py</path>
      <name>loss.py</name>
      <extension>.py</extension>
      <size_bytes>2490</size_bytes>
      <modified>2026-01-17T17:40:34.642054</modified>
      <encoding>utf-8</encoding>
      <lines>68</lines>
      <content><![CDATA[
"""
æŸå¤±å‡½æ•°
"""
import numpy as np
import my_deep_lib as cuda_lib
from basic_operator import Op, Value

class CrossEntropyLossOp(Op):
    """äº¤å‰ç†µæŸå¤±"""
    
    def compute(self, logits_data: np.ndarray, labels_data: np.ndarray) -> np.ndarray:
        """
        logits_data: [batch, num_classes] (æœªç»è¿‡ softmax)
        labels_data: [batch] (ç±»åˆ«ç´¢å¼•)
        """
        # å…ˆåœ¨ CUDA ä¸Šåš softmax
        cuda_logits = cuda_lib.Tensor.from_numpy(logits_data, cuda_lib.Device.gpu(0))
        cuda_probs = cuda_lib.Tensor(list(logits_data.shape), cuda_lib.Device.gpu(0))
        cuda_lib.softmax_forward(cuda_logits, cuda_probs)
        
        # è®¡ç®—äº¤å‰ç†µ
        cuda_labels = cuda_lib.Tensor.from_numpy(labels_data, cuda_lib.Device.gpu(0))
        loss = cuda_lib.cross_entropy_loss(cuda_probs, cuda_labels)
        
        # ä¿å­˜ probs ç”¨äºåå‘ä¼ æ’­
        self.probs = cuda_probs.to_numpy()
        
        return np.array(loss, dtype=np.float32)
    
    def gradient(self, out_grad: Value, node: Value) -> tuple:
        """
        äº¤å‰ç†µ + softmax çš„èåˆæ¢¯åº¦
        grad_logits = (probs - one_hot(labels)) / batch_size
        """
        _, labels_node = node.inputs
        labels_data = labels_node.realize_cached_data()
        
        # è½¬æ¢ä¸º CUDA Tensor
        cuda_probs = cuda_lib.Tensor.from_numpy(self.probs, cuda_lib.Device.gpu(0))
        cuda_labels = cuda_lib.Tensor.from_numpy(labels_data, cuda_lib.Device.gpu(0))
        cuda_grad_logits = cuda_lib.Tensor(list(self.probs.shape), cuda_lib.Device.gpu(0))
        
        # è°ƒç”¨ CUDA backward kernel
        cuda_lib.cross_entropy_backward(cuda_probs, cuda_labels, cuda_grad_logits)
        
        grad_logits_np = cuda_grad_logits.to_numpy()
        
        # ä¹˜ä»¥ out_gradï¼ˆé€šå¸¸æ˜¯ 1ï¼‰
        out_grad_data = out_grad.realize_cached_data()
        grad_logits_np = grad_logits_np * out_grad_data
        
        grad_logits = Value.make_const(grad_logits_np, requires_grad=False)
        grad_labels = None  # labels ä¸éœ€è¦æ¢¯åº¦
        
        return (grad_logits, grad_labels)

def cross_entropy_loss(logits: Value, labels: Value) -> Value:
    """
    äº¤å‰ç†µæŸå¤±ï¼ˆèåˆäº† Softmaxï¼‰
    
    Args:
        logits: [batch, num_classes] (æœªç»è¿‡ softmax çš„åŸå§‹è¾“å‡º)
        labels: [batch] (ç±»åˆ«ç´¢å¼•ï¼Œå¦‚ [0, 2, 1, 3, ...])
    
    Returns:
        loss: æ ‡é‡
    """
    return CrossEntropyLossOp()(logits, labels)
]]></content>
    </file>

    <file index="20">
      <path>python/nn/module.py</path>
      <name>module.py</name>
      <extension>.py</extension>
      <size_bytes>1502</size_bytes>
      <modified>2026-01-17T17:29:27.690229</modified>
      <encoding>utf-8</encoding>
      <lines>57</lines>
      <content><![CDATA[
"""
Module åŸºç±»ï¼ˆç±»ä¼¼ PyTorch çš„ nn.Moduleï¼‰
"""
from typing import List, Iterator
from basic_operator import Value

class Parameter(Value):
    """å¯è®­ç»ƒå‚æ•°ï¼ˆç»§æ‰¿è‡ª Valueï¼‰"""
    pass

def _unpack_params(value: object) -> List[Value]:
    """é€’å½’æå–æ‰€æœ‰å‚æ•°"""
    if isinstance(value, Parameter):
        return [value]
    elif isinstance(value, Module):
        return value.parameters()
    elif isinstance(value, dict):
        params = []
        for k, v in value.items():
            params += _unpack_params(v)
        return params
    elif isinstance(value, (list, tuple)):
        params = []
        for v in value:
            params += _unpack_params(v)
        return params
    else:
        return []

class Module:
    """ç¥ç»ç½‘ç»œæ¨¡å—åŸºç±»"""
    
    def __init__(self):
        self.training = True
    
    def parameters(self) -> List[Parameter]:
        """è¿”å›æ‰€æœ‰å¯è®­ç»ƒå‚æ•°"""
        return _unpack_params(self.__dict__)
    
    def zero_grad(self):
        """æ¸…ç©ºæ¢¯åº¦"""
        for p in self.parameters():
            p.grad = None
    
    def train(self):
        """è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼"""
        self.training = True
    
    def eval(self):
        """è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼"""
        self.training = False
    
    def __call__(self, *args, **kwargs):
        return self.forward(*args, **kwargs)
    
    def forward(self, *args, **kwargs):
        raise NotImplementedError("Subclasses must implement forward()")
]]></content>
    </file>

    <file index="21">
      <path>python/utils.py</path>
      <name>utils.py</name>
      <extension>.py</extension>
      <size_bytes>2738</size_bytes>
      <modified>2026-01-17T17:18:08.154425</modified>
      <encoding>utf-8</encoding>
      <lines>74</lines>
      <content><![CDATA[
"""
æ­¤æ¬¡ä½œä¸šå€Ÿé‰´å’Œå‚è€ƒäº†Needleé¡¹ç›® https://github.com/dlsyscourse/lecture5
æœ¬æ–‡ä»¶æˆ‘ä»¬ç»™å‡ºå¯èƒ½ä¼šç”¨åˆ°çš„ä¸€äº›å·¥å…·å‡½æ•°
ä¸ºé˜²æ­¢äº¤å‰å¼•ç”¨ï¼Œä½ å¯ä»¥æŠŠéœ€è¦çš„å‡½æ•°å¤åˆ¶åˆ°ä½ çš„ä»£ç ä¸­
"""

import math
from device import cpu
from task1_operators import Tensor

def rand(*shape, low=0.0, high=1.0, device=None, dtype="float32", requires_grad=False):
    """Generate random numbers uniform between low and high"""
    device = cpu() if device is None else device
    array = device.rand(*shape) * (high - low) + low
    return Tensor(array, device=device, dtype=dtype, requires_grad=requires_grad)


def randn(*shape, mean=0.0, std=1.0, device=None, dtype="float32", requires_grad=False):
    """Generate random normal with specified mean and std deviation"""
    device = cpu() if device is None else device
    array = device.randn(*shape) * std + mean
    return Tensor(array, device=device, dtype=dtype, requires_grad=requires_grad)


def constant(*shape, c=1.0, device=None, dtype="float32", requires_grad=False):
    """Generate constant Tensor"""
    device = cpu() if device is None else device
    array = device.ones(*shape, dtype=dtype) * c  # note: can change dtype
    return Tensor(array, device=device, dtype=dtype, requires_grad=requires_grad)


def ones(*shape, device=None, dtype="float32", requires_grad=False):
    """Generate all-ones Tensor"""
    return constant(
        *shape, c=1.0, device=device, dtype=dtype, requires_grad=requires_grad
    )


def zeros(*shape, device=None, dtype="float32", requires_grad=False):
    """Generate all-zeros Tensor"""
    return constant(
        *shape, c=0.0, device=device, dtype=dtype, requires_grad=requires_grad
    )


def randb(*shape, p=0.5, device=None, dtype="bool", requires_grad=False):
    """Generate binary random Tensor"""
    device = cpu() if device is None else device
    array = device.rand(*shape) <= p
    return Tensor(array, device=device, dtype=dtype, requires_grad=requires_grad)


def one_hot(n, i, device=None, dtype="float32", requires_grad=False):
    """Generate one-hot encoding Tensor"""
    device = cpu() if device is None else device
    return Tensor(
        device.one_hot(n, i.numpy(), dtype=dtype),
        device=device,
        requires_grad=requires_grad,
    )


def zeros_like(array, *, device=None, requires_grad=False):
    device = device if device else array.device
    return zeros(
        *array.shape, dtype=array.dtype, device=device, requires_grad=requires_grad
    )


def ones_like(array, *, device=None, requires_grad=False):
    device = device if device else array.device
    return ones(
        *array.shape, dtype=array.dtype, device=device, requires_grad=requires_grad
    )
]]></content>
    </file>

  </files>

</project>
